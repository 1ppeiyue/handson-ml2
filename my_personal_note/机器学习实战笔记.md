---
tags:
  - DataScience
  - "#DataScience-MachineLearning"
---
> 作为[[视频笔记]]的进一步学习
## 第一部分 机器学习的基础知识
### 第一章 机器学习概览
>机器学习并不是遥不可及的未来科技，而是一种已经广泛应用的技术 —— 从光学字符识别OCR到垃圾邮件过滤器
#### 1.1 什么是机器学习
用不同的语言解释了机器学习的定义：
1. **机器学习是一门通过编程让计算机从数据中进行学习的科学（和艺术）。**
2. **机器学习是一个研究领域， 让计算机无须进行明确编程就具备学习能力。**
    - 机器学习的本质：让计算机拥有类似人类的学习能力， 而不依赖于预先确定的规则编程。
3. **更工程化的概念：**
	- **亚瑟·萨缪尔 (Arthur Samuel)：** 机器学习指的是一个计算机程序利用经验 E 来进行学习任务 T。 如果随着经验 E 不断增长写作， 任务 T 在性能指标 P 上有所提高， 这便称为机器学习。
	- **汤姆·米切尔 (Tom Mitchell)：** “一个计算机程序利用经验 E 来学习任务 T， 性能是 P， 如果针对任务 T 的性能 P 随着经验 E 不断增长， 则称为机器学习。”
4. **举例： 垃圾邮件过滤器**
    - 根据垃圾邮件（比如，用户标记的垃圾邮件）和普通邮件（非垃圾邮件，也称 作ham）学习标记垃圾邮件。
	    - 系统用来进行学习的样例称作训练集。
	    - 每个训练样例称作训练实例（或样本）。
	- 在这个示例中，任务T就是标记 新邮件是否是垃圾邮件，经验E是训练数据，性能P需要定义。例如，可以使用正确分类邮件的比例。这个性能指标称为准确率，通常用在分类任务中。

#### 1.2 为什么使用机器学习
> 通过一个垃圾邮件过滤器的例子， 用来对比传统的编程方法和基于机器学习的方案， 重点突出机器学习的优点：更灵活、 更易维护和更精确。
- **1. 传统编程方法**
    用传统的编程方法编写垃圾邮件过滤器的步骤：
    - **研究问题：** 要了解垃圾邮件的特点， 比如哪些词语和短语经常出现 (例如 "4U"， "credit card"， “free"， “amazing" 等等）。
    - **编写规则：** 根据前面观察到的模式， 为每个模式编写一个检测算法。 例如： 如果邮件标题中包含“4U” 这个词语， 就标记为垃圾邮件。
    - **评估：** 测试这个程序， 看它的效果是否足够好。
    - **分析錯誤：** 检查程序判断错误的邮件， 看看是哪些规则出了问题， 然后修改这些规则。
    - **重复上述步骤：** 直到程序效果足够好才能发布。  
    - 作者最后用“你的程序很可能会變成一长串复杂的规则”来说明了传统编程方法的局限性。 当问题变得更复杂的时候， 这个方法需要编写大量的代码， 而且很难维护。
    
- **2. 基于机器学习的方法**
    基于机器学习技术的垃圾邮件过滤器的方案：
    - **它会自动学习那些作为垃圾邮件预测因素的词语和短语。**
    - **它会通过对比正常邮件， 学习识别垃圾邮件中反复出现的词语模式。**
    - **这个方法更简单、 更易维护， 而且它更精确。**
    
- **3. 比较和对比**
    - **传统方法的问题：** 當垃圾邮件发送者不断变化他们的发送方式 (例如: 将 “4U" 改成 "For U") 的时候， 传统的垃圾邮件过滤器就需要不断修改规则， 才能识别出新的垃圾邮件。
    - **机器学习的优势：** 机器学习方法可以自动适应词语的变化， 不需要持续修改规则。

#### 1.3 机器学习的应用示例

| 主题                                                                | 应用场景                                                                                                               | 解决方式                                                                                   | 技术                                                                                                                                                                                                         | 图片                                                                                                  |
| ------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |:----------------------------------------------------------------------------------------------------- |
| 分析生产线上的产品图像来对产品进行自动分类                          | 在生产线上，需要对产品进行质量检测，判断产品合格与否                                                                   | 使用卷积神经网络 (CNN) 进行图像分类                                                        | CNN 是图像识别领域最常用的深度学习技术之一                                                                                                                                                                   |                                                                                                       |
| 通过脑部扫描发现肿瘤                                                | 医生用脑部扫描来诊断疾病，判断是否存在肿瘤                                                                             | 使用语义分割技术， 对脑部扫描图像中的每个像素进行分类， 定位肿瘤的精确位置。               | CNN 也是该领域的主要技术                                                                                                                                                                                     |                                                                                                       |
| 自动分类新闻                                                        | 对互联网上的海量新闻进行分类， 例如： 体育、 娱乐、 财经等等                                                           | 使用自然语言处理技术 (NLP), 其中包括文本分类， 可以使用循环神经网络、 CNN 或者 Transformer | 循环神经网络 (RNN)、 CNN 或者 Transformer 都是 NLP 中常用的深度学习技术                                                                                                                                      | 图像识别系统根据新闻的标题和内容文字进行处理， 然后在图像中给出标签， 例如 “体育”，“财经”，“娱乐”     |
| 论坛中自动标记恶评                                                  | 对互联网论坛上的用户评论进行自动识别， 判断评论是否为恶评。                                                            | 使用 NLP 技术中的文本分类                                                                  | 使用相同的 NLP 技术和前面自动分类新闻中提到的 RNN、CNN 或者 Transformer                                                                                                                                      | 用户界面， 系统会根据文字自动判断用户评论是否为恶评， 然后在恶评旁边标记图标， 例如：红色 “差评” 标签 |
| 自动对长文章做总结                                                  | 帮助用户快速了解长篇文章的要点                                                                                         | 使用 NLP 技术中的文本总结                                                                  | 仍然使用 RNN、 CNN， 或者 Transformer                                                                                                                                                                        |                                                                                                       |
| 创建一个聊天机器人或者个人助理                                      | 让机器能够像人一样进行对话， 处理用户的请求                                                                            | 使用自然语言处理 (NLP) 以及相关的机器学习技术， 才能让机器像人一般进行对话、 完成任务      | NLP 涵盖了很多技术， 包括语音识别、 自然语言理解、 自然语言生成、 语义分析等等， 需要使用到一些先进的深度学习技术， 例如：循环神经网络 (RNN)， Transformer, 卷积神经网络 (CNN), 注意力机制 (Attention） 等等 |                                                                                                       |
| 基于很多性能指标来预测公司下一年的收入                              | 利用公司过去的数据 (例如 利润、 收入、 成本等) 来预测公司下一年的收入                                                  | 回归模型                                                                                   | 线性回归或者其他回归模型 (例如 SVM 回归， 随机森林回归， 人工神经网络等等)                                                                                                                                   |                                                                                                       |
| 让应用对语音命令做出反应                                            | 让用户可以通过语音来操控程序或者设备                                                                                   | 使用语音识别技术                                                                           | 该领域也需要用到 RNN 以及 Transformer 等深度学习技术                                                                                                                                                         | 用户对着手机语音说出指令， 然后手机屏幕上显示响应结果                                                 |
| 检测信用卡欺诈                                                      | 自动识别信用卡交易中是否出现了欺诈行为                                                                                 | 使用异常检测技术                                                                           | 异常检测技术有很多种， 比如：基于统计方法的异常检测、 基于距离的异常检测、 基于密度的方法等等。 需要选择最适合信用卡欺诈检测的异常检测方法                                                                   | 可以用图片形式展现出各种类型的信用卡交易数据， 然后用高亮颜色突出标记出有欺诈风险（异常数据点）       |
| 基于客户的购买记录来对客户进行分类， 对每一类客户设计不同的市场策略 | 通过收集用户数据， 将用户进行分类， 例如 “高价值用户”，“低价值用户” 等等， 然后根据不同的用户群体， 指定不同的营销策略 | 使用聚类技术                                                                               | 聚类技术也比较多样， 例如 : K-means 算法， 层次聚类等等                                                                                                                                                      | 用图片展示用户群体的分类 (可以用不同颜色代表不同类型的用户群体)                                       |
| 用清晰而有洞察力的图表来表示复杂的高维数据集                        | 将难以直接理解的高维数据可视化， 使数据更容易被理解                                                                    | 使用数据可视化技术                                                                         | 常用的可视化技术包括：散点图、 折线图、 柱状图、 饼图等等， 也可以使用更专业的工具， 例如 Tableau， Power BI 来进行可视化                                                                                    | 展示一个高维数据集的可视化结果， 比如在二维空间里展示多个维度的关系， 并解释各个维度的含义            |
| 基于以前的购买记录给客户推荐可能感兴趣的产品                        | 个性化推荐系统， 例如： 电商平台中经常看到的 “猜你喜欢”                                                                | 使用推荐系统技术                                                                           | 推荐系统技术也比較庞杂， 通常需要结合用户画像， 商品信息， 历史行为数据等等， 并使用机器学习算法， 例如： 协同过滤， 基于内容推荐， 或者使用更加复杂的深度学习模型等等                                       |                                                                                                       |
#### 1.4 机器学习系统的类型
>可以根据不同的分类标准， 对机器学习系统进行分类

>1.根据是否在人类的监督下学习

| 分类    | 内容                                            | 示例                                                                                                            |
| ----- | --------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |
| 有监督学习 | 算法在训练的时候需要有 “标签”。也就是在每个数据样本中， 都包含了预先定义好的正确答案。 | 一个模型， 以识别手写数字图像。 每个图像都会有一个对应的标签， 比如 “2”，“5”，“9” 等等。<br>2.训练一个模型， 用来预测房屋的价格。 每条房屋信息都会有一个对应的房屋价格标签。             |
| 无监督学习 | 算法在训练的时候没有 “标签”。算法需要自己从数据中发现规律和模式。            | 1.聚类 (Clustering)： 将数据集合分成不同的组， 每个组中的数据具有相似特征。<br>2.降维 (dimensionality reduction): 将高维数据转换成低维数据， 同时保留数据的关键特征。 |
| 半监督学习 | 介于有监督学习和无监督学习之间。在训练的时候既有带标签的数据，也有没有标签的数据。     | 可以利用大量的无标签数据帮助算法更好地理解数据的分布和规律。<br>然后用少量带标签的数据对模型进行微调。                                                         |
| 强化学习  | 算法通过不断与环境进行交互来学习                              | 1.训练一个程序， 它可以玩游戏 (比如 围棋)。 程序通过不断地尝试， 学习哪些策略可以取得胜利。<br>2.训练一个机器人， 它可以学习如何执行某项任务， 例如 走路、 抓取物体等等。                |

>2.根据是否可以动态地进行增量学习

| 分类   | 内容                                        | 示例                                                                                         |
| ---- | ----------------------------------------- | ------------------------------------------------------------------------------------------ |
| 在线学习 | 算法可以接收新的数据，并且更新模型，而不用重新训练整个模型。适合数据不断变化的情况 | 1.垃圾邮件过滤器：每当分析了一封新邮件，都会对模型进行更新，以提高识别新类型垃圾邮件的能力。<br>2.推荐系统：每当用户点击了一个商品，就更新模型，更好地推荐用户感兴趣的商品。 |
| 批量学习 | 整个训练过程需要使用所有数据。如果要添加新数据，需要重新训练整个模型。       | 对于有很多数据， 但是数据不会频繁更新的情况， 可以使用批量学习                                                           |


>3.根据是只将新的数据点和已知数据点进行匹配， 还是像科学家一样对训练数据进行模式检测进而建立一个预测模型

| 分类      | 内容                                 | 示例                                                                                                               |
| ------- | ---------------------------------- | ---------------------------------------------------------------------------------------------------------------- |
| 基于实例的学习 | 算法将新的数据点与已知数据点进行比较， 然后做出预测         | 1.k-Nearest Neighbors (KNN) 算法 - 它将新的数据点与最相似的 k 个已知数据点进行比较， 然后根据这些邻居的类别来判断该数据点的类别。<br>2.基于实例的机器学习应用， 例如： 识别手写数字。 |
| 基于模型的学习 | 算法根据训练数据学习一个模型（公式或函数）， 然后使用模型来做出预测 | 逻辑回归 (Logistic Regression), 决策树， 神经网络等等， 这些模型可以根据数据学习到更复杂的特征和关系。                                                 |
#### 1.5 机器学习的主要挑战

| 挑战         | 内容                                                                | 解决方案                                                           |
| ---------- | ----------------------------------------------------------------- | -------------------------------------------------------------- |
| 数据量不足      | 训练数据不够，模型无法学习有效的模式                                                | 1.数据增强: 生成更多数据 (旋转、翻转、添加噪声...)<br>2.小样本学习: 用少量数据训练模型。          |
| 训练数据不具有代表性 | 训练数据不能反映实际情况， 导致模型无法泛化到新数据                                        | **收集更具代表性的数据**: 确保训练数据包含足够的真实场景                                |
| 数据质量差      | 数据中有错误、缺失值、重复数据等问题， 影响模型准确性                                       | ==**数据清洗**==: 清理数据中的错误和噪声                                      |
| 无关特征       | 数据集中包含与目标无关的特征， 降低模型效率和预测能力                                       | 1.**特征选择**: 选取与目标相关的特征。<br>2.==**特征工程**:== 对特征进行加工处理。          |
| 过拟合&欠拟合    | 1.**过拟合**: 模型对训练数据拟合过好， 对测试数据拟合很差。<br>2.**欠拟合**: 对训练数据和测试数据都拟合很差。 | ![[Pasted image 20241226205906.png]]                           |
| 数据不匹配      | 训练数据和测试数据来自不同的分布，模型无法有效泛化                                         | 1.**数据预处理**: 对训练数据和测试数据进行统一处理。<br>2.**迁移学习**: 利用在其他数据集上训练好的模型。 |
#### 1.6 关键概念

| 概念                       | 内容                                           | 示例                                                                                                                                           |
| ------------------------ | -------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| 训练集 Training Set         | 训练集包含用来训练模型的数据                               |                                                                                                                                              |
| 测试集 Test Set             | 测试集包含用来评估模型性能的数据                             | 的数据在训练过程中没有被模型使用。<br>使用测试集来评估模型的泛化能力 (generalization ability)， 即 模型在从未见过的数据上能否表现良好                                                           |
| 验证集 Vaildation Set       | 验证集包含用来调优 (tuning) ==模型超参数==的数据              | 数 (hyperparameters) 是指在训练模型*之前需要设置好的参数*， 例如：神经网络中的层数， 每层的节点数， 学习率等等                                                                          |
| 特征工程 Feature Engineering | 特征工程是指将原始数据转换为模型可以理解和利用的特征的过程，它扮演着重要的角色      | 本数据转换成词汇向量 (word embeddings)<br>2.将图像数据转换成特征向量 (feature vectors)                                                                             |
| 模型选择 Model Selection     | 模型选择是指选择最适合当前任务的模型                           | 1.线性回归 - 适合解决回归问题， 预测连续的值。<br>2.逻辑回归 - 适合解决分类问题， 预测离散的值。<br>3.决策树 - 适合解决分类和回归问题， 可以学习数据的复杂关系。<br>4.神经网络 - 适合解决复杂的机器学习问题， 比如： 图像识别， 自然语言处理等等。 |
| 正则化 Regularization       | 正则化是指在模型训练的过程中，对模型参数进行约束，通过降低模型的复杂度，避免模型过度拟合 | ![[Pasted image 20241226212941.png]]                                                                                                         |
| 过拟合 Overfitting          | 过拟合是指模型对训练数据的拟合程度很高，但是对测试数据的拟合程度很低           | 过拟合的原因：模型过渡适应了训练数据，导致模型学习到了训练数据中的一些噪声，它不能很好地泛化到测试数据                                                                                          |
| 欠拟合 Underfitting         | 欠拟合是指模型对训练数据和测试数据的拟合程度都很低                    | 欠拟合的原因：模型过于简单，无法学习到数据的真实规律                                                                                                                   |
#### 1.7 练习题
1. *如何定义机器学习？*

机器学习是关于构建可以从数据中学习的系统。学习意味着在一 定的性能指标下，在某些任务上会变得越来越好。

2. *机器学习在哪些问题上表现突出，你能给出四种类型吗？*

机器学习非常适合没有算法解答的复杂问题，它可以替代一系列 需要手动调整的规则，来构建适应不断变化的环境的系统并最终帮助人 类（例如，数据挖掘）。

3. *什么是被标记的训练数据集？*

带标签的训练集是一个包含每个实例所需解决方案（也称为标签）的训练集。

4. *最常见的两种监督学习任务是什么？*

回归和分类

5. *你能举出四种常见的无监督学习任务吗？*

聚类、可视化、降维和关联规则学习

6. *要让一个机器人在各种未知的地形中行走，你会使用什么类型的机器学习算法？

强化学习

7. *要将顾客分成多个组，你会使用什么类型的算法？

如果你不知道如何定义组，则可以使用聚类算法（无监督学习）将客户划分为相似客户集群。但是，如果你知道你想要拥有哪些组，那么可以将每个组的许多实例提供给分类算法（有监督学习），并将所有客户分类到这些组中。

8. *你会将垃圾邮件检测的问题列为监督学习还是无监督学习？

垃圾邮件检测是一个典型的有监督学习问题：向该算法提供许多电子邮件及其标签（垃圾邮件或非垃圾邮件）

9. *什么是在线学习系统？

与批量学习系统相反，在线学习系统能够进行增量学习。这使得 它能够快速适应不断变化的数据和自动系统，并能够处理大量数据

10. *什么是核外学习？

核外算法可以处理无法容纳在计算机主内存中的大量数据。核 外学习算法将数据分成小批量，并使用在线学习技术从这些小批量数据 中学习

11. *什么类型的学习算法依赖相似度来做出预测？

基于实例的学习系统努力通过死记硬背来学习训练数据。然 后，当给定一个新的实例时，它将使用相似性度量来查找最相似的实 例，并利用它们来进行预测。

12. *模型参数与学习算法的超参数之间有什么区别？

一个模型具有一个或多个模型参数，这些参数确定在给定一个新实例的情况下该模型将预测什么（例如，线性模型的斜率）。一种学习算法试图找到这些参数的最优值，以使该模型能很好地泛化到新实例。超参数是学习算法本身的参数，而不是模型的参数（例如，要应用正则化的数量）

13. *基于模型的学习算法搜索的是什么？它们最常使用的策略是什么？它们如何做出预测？

基于模型的学习算法搜索模型参数的最优值，以便模型可以很 好地泛化到新实例。我们通常通过最小化成本函数来训练这样的系统， 该函数测量系统对训练数据进行预测时有多不准确，如果对模型进行了 正则化则对模型复杂性要加上惩罚。为了进行预测，我们使用学习算法 找到的模型参数值，再将新实例的特征输入到模型的预测函数中。

14. *你能给出机器学习中的四个主要挑战吗？

机器学习中的一些主要挑战是数据的缺乏、数据质量差、数据的代表性不足、信息量不足、模型过于简单而欠拟合训练数据以及模型 过于复杂而过拟合数据。

15. *如果模型在训练数据上表现很好，但是应用到新实例上的泛化结果却很糟糕，是怎么回事？能给出三种可能的解决方案吗？

如果模型在训练数据上表现出色，但在新实例上的泛化效果很 差，则该模型可能会过拟合训练数据（或者我们在训练数据上非常幸运）。过拟合的可能解决方法是获取更多数据、简化模型（选择更简单的算法，减少使用的参数或特征的数量，或对模型进行正则化）或减少训练数据中的噪声。

16. *什么是测试集，为什么要使用测试集？

测试数据集是用于在启动生产环境之前，估计模型在新实例上产生的泛化误差。

17. *验证集的目的是什么？

验证集是用于比较模型。这样就可以选择最佳模型并调整超参数

18. *什么是train-dev集，什么时候需要它，怎么使用？

当训练数据集与验证数据集和测试数据集中使用的数据之间不匹配时，可以使用train-dev集（该数据集应始终与模型投入生产环境后使用的数据尽可能接近）。train-dev集是训练集的一部分（模型未在其上训练过）。该模型在训练集的其他部分上进行训练，并在traindev集和验证集上进行评估。如果模型在训练集上表现良好，但在train-dev集上表现不佳，则该模型可能过拟合训练集。如果它在训练集和train-dev集上均表现良好，但在验证集上却表现不佳，那么训练数据与验证数据和测试数据之间可能存在明显的数据不匹配，你应该尝试改善训练数据，使其看起来更像验证数据和测试数据。

19. *如果你用测试集来调超参数会出现什么错误？

如果使用测试集来调整超参数，则可能会过拟合测试集，而且所测得的泛化误差会过于乐观（你可能会得到一个性能比预期差的模型）。

### 第二章 端到端的机器学习项目
#### 2.1 使用真实数据
流行的开放数据存储库：
UC Irvine Machine Learning Repository（[http://archive.ics.uci.edu/ml/]()）
Kaggle datasets（[https://www.kaggle.com/datasets]()）
Amazon’s AWS datasets（[http://aws.amazon.com/fr/datasets/]()）

元门户站点（它们会列出开放的数据存储库）：
Data Portals（[http://dataportals.org/]()）
OpenDataMonitor（[http://opendatamonitor.eu/]()）
Quandl（[http://quandl.com/]()）

其他一些列出许多流行的开放数据存储库的页面：
Wikipedia’s list of Machine Learningdatasets（[https://goo.gl/SJHN2k]()）
Quora.com（[http://goo.gl/zDR78y]()）
The datasets subreddit（[https://www.reddit.com/r/datasets]()）

公开数据集:
	Kaggle (https://www.kaggle.com/): 提供各种领域的数据集。
	UCI 机器学习库 (https://archive.ics.uci.edu/ml/index.php): 提供了各种经典的机器学习数据集。
	Google 数据集搜索 (https://datasetsearch.research.google.com/): 提供搜索不同类型数据集的功能。
	Amazon 数据集 (https://registry.opendata.aws/): 亚马逊提供的公开数据集库。

网页抓取: 使用 Python 的库（例如 BeautifulSoup 或 Scrapy） 从网站上提取所需数
据。

API 调用: 使用 API 获取数据， 例如：
	天气 API（例如， OpenWeatherMap https://openweathermap.org/api）
	交通 API（例如， Google Maps API）
	金融 API（例如， Yahoo Finance API）

数据库: 连接数据库， 使用 SQL 语句获取数据。

传感器数据: 利用传感器收集的数据， 例如 IoT 设备、 运动跟踪器、 环境监测设备 等等。

#### 2.2 观察大局

|              | 内容                                                                                                                                                                                                   |
| ------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 要求           | 使用加州人口普查的数据建立起加州的房价模型，根据所有其他指标，*预测任意区域的房价中位数*                                                                                                                                                        |
| 2.2.1 框架问题   | 问题: 这个项目最终想要解决什么问题？我们如何使用模型的结果？我们期望获得哪些效益？”<br>原因: 明确项目的业务目标，才能更好地确定解决方法。<br><br>学习类型: 是有监督学习、无监督学习，还是强化学习？<br>任务类型: 是分类任务、回归任务，还是其他任务？<br>学习方式: 是使用批量学习，还是在线学习？![[Pasted image 20241227175635.png]] |
| 2.2.2 选择性能指标 | ![[Pasted image 20241227180824.png]]![[Pasted image 20241227185001.png]]                                                                                                                             |
| 2.2.3 检查假设   | ![[Pasted image 20241227181402.png]]                                                                                                                                                                 |
#### 2.3 获取数据
##### 2.3.1 创建工作区
- 安装 Python 以及必要的 Python 库，例如 Jupyter、 NumPy、 pandas、 Matplotlib 和 Scikit-Learn。
- 使用 virtualenv 创建隔离环境， 避免库版本冲突。
- 启动 Jupyter Notebook。
##### 2.3.2 下载数据
- 编写 `fetch_housing_data` 函数， 自动下载并解压缩 `housing.tgz` 文件。
- 编写 `load_housing_data` 函数， 使用 pandas 加载 `housing.csv` 数据文件到 DataFrame 对象中。
##### 2.3.3 快速查看数据结构
- 使用 `head()` 方法查看数据的前几行。
- 使用 `info()` 方法查看数据的基本信息，例如总行数、属性类型、非空值数量等等。
- 使用 `value_counts()` 方法查看分类属性的不同值出现的次数。
- 使用 `describe()` 方法查看数值属性的统计摘要，例如 mean、 std 、 min、 max、 25%、 50%、 75% 等。
- 使用 `hist()` 方法绘制每个数值属性的直方图，直观地观察数据分布。
##### 2.3.4 创建数据集
- 为了避免数据泄露， 需要在数据探索之前分离出一个测试集。
- 使用 `split_train_test` 函数随机将数据集分成训练集和测试集。
- 为了确保数据分割的稳定性， 可以使用 `crc32` 函数对数据的唯一标识符进行哈希运算， 以确定数据实例是否应该被放入测试集。
- 为了方便起见，可以使用 Scikit-Learn 提供的 `train_test_split` 函数， 它提供类似的功能， 并且支持一次性分割多个数据集。
##### 2.3.5 分层抽样
- 问题: 当数据集大小有限时， 随机抽样可能会产生抽样偏差。
- 解决方案: 使用分层抽样， 按照某个重要特征进行数据分组， 并从每个组中抽取一定比例的样本。
- 步骤:
	选择重要特征， 例如 `median_income` 进行分层抽样。
	使用 `pd.cut()` 将连续特征转化成离散特征， 便于分层。
	使用 `StratifiedShuffleSplit` 函数进行分层抽样。

#### 2.4 从数据探索和可视化中获得洞见
##### 2.4.1 创建数据副本
- 为了避免修改原始训练集， 需要创建一个副本进行探索。
- `housing.copy()` 创建一个 DataFrame 对象（housing）的副本。
##### 2.4.2 可视化地理数据
- 使用 `plot()` 函数将 longitude 和 latitude 绘制成散点图， 直观地观察数据的空间分布。
- 调整 `alpha` 参数来控制数据的透明度， 可以更明显地看到数据点的聚集区域。
##### 2.4.3 寻找相关性
- 使用 corr() 方法计算各属性之间的相关系数， 来分析属性之间是否存在线性关系。
- corr_matrix["median_house_value"].sort_values(ascending=False) 显示每个属性与 median_house_value 之间的相关性， 了解哪些属性可能对 median_house_value 具有较强的预测能力。
- 使用 scatter_matrix() 函数可以绘制出所有数值属性之间的散点图， 更直观地观察它们的相关关系
##### 2.4.4 试验不同属性的组合
- 创建新的属性组合， 来挖掘原有属性之间隐含的联系。
- `housing["rooms_per_household"] = housing["total_rooms"]/housing["households]` 创建 `rooms_per_household` 属性， 表示每个家庭的房间数。
- 再次计算相关系数， 发现 bedrooms_per_room 和 rooms_per_household 属性与 median_house_value 之间的相关性更高， 说明新属性组合可能具有更好的预测能力。
#### 2.5 机器学习算法的数据准备
目标: 对原始数据进行清洗、转换等操作， 准备适合机器学习算法使用的格式， 为后续模型训练和评估打下基础。
##### 2.5.1 分离预测器和标签
- 将训练集中的 median_house_value 属性（即标签） 从 housing DataFrame 中分离出来。
- `housing = strat_train_set.drop("median_house_value", axis=1)`
- `housing_labels = strat_train_set["median_house_value"].copy()`
##### 2.5.2 数据清理
<mark style="background: #ADCCFFA6;">处理缺失值</mark>：
- 方法一：删除缺失值所在的行。
- 方法二：删除整个属性（即包含缺失值的特征）
- 方法三：用 median、 mean 等统计量进行填充。

Scikit-Learn 提供的 SimpleImputer 类可以方便地填充缺失值
- 创建 SimpleImputer 实例， 指定填充策略 (例如 strategy="median")
- 使用 fit() 方法让 imputer 学习训练集中的 median 值。
- 使用 transform() 方法将 imputer 应用于训练集， 填充缺失值。
##### 2.5.3 处理文本和分类属性
- 将文本类别值转换成数字值， 可以使用 OrdinalEncoder 类。
![[Pasted image 20241227200815.png]]
- 将数值类别值转换为独热向量 (one-hot encoding)， 可以使用 OneHotEncoder 类。
![[Pasted image 20241227200842.png]]
- 如果类别属性具有大量类别值， 可以选择使用其它方法代替 one-hot encoding， 比如 用数值特征代替类别特征， 或使用 embedding 技术 (可在训练过程中学习每个类别的向量表征)。
##### 2.5.4 自定义转换器
- 创建 CombinedAttributesAdder 类， 用来组合一些属性并生成新的属性。
- 继承 BaseEstimator 和 TransformerMixin 类， 确保自定义转换器符合 Scikit-Learn 的规范。
- 实现 fit 、 transform 和 fit_transform 方法。
- 使用自定义转换器可以实现更灵活的数据准备过程。
![[Pasted image 20241227201143.png]]
##### 2.5.5 特征缩放
- 特征缩放可以提高某些机器学习算法 (例如基于距离的算法) 的性能。
![[Pasted image 20241227201205.png]]
- 两种常用的特征缩放方法是<mark style="background: #FFB8EBA6;"> 最小 - 最大缩放</mark> 和 <mark style="background: #FFB8EBA6;">标准化</mark>。
![[Pasted image 20241227201239.png]]
- Scikit-Learn 提供 MinMaxScaler 和 StandardScaler 类， 可以方便地进行特征缩放。
- 注意：只对训练集进行缩放， 之后再将缩放后的结果应用于测试集和新数据。
##### 2.5.6 转换流水线
- 使用 `Pipeline` 类将数据预处理步骤串联起来， 形成一个 pipeline， 便于 fit 和 transform 的操作。
- 构造 Pipeline 实例， 通过 name/estimator 元组列表指定数据预处理步骤的顺序。
- Pipeline 的 fit() 方法会按顺序调用 fit_transform() 方法， 将上一步的输出作为下一步骤的 input.
- 使用 `ColumnTransformer` 将不同的转换方法应用于不同类型的属性， 例如， 对 numerical 属性进行 SimpleImputer 和 StandardScaler ， 对 categorical 属性进行 OneHotEncoder。
![[Pasted image 20241227201309.png]]
#### 2.6 选择和训练模型
##### 2.6.1 训练和评估
1. **选择模型**:
    - 最简单的模型是线性回归模型 (LinearRegression)
    - 另一个比较常用的模型是决策树模型 (DecisionTreeRegressor), 它能学习更复杂的关系。
2. **训练模型**:
    - 将准备好的数据 (`housing_prepared`) 喂给模型， 它就可以根据 `housing_labels` 学习房屋价格的规律了。
3. **评估模型**:
    - 使用训练集的一部分数据来评估模型的好坏（看它学得怎样）。
    - 最常用的评估指标是 RMSE（Root Mean Squared Error）， 它衡量的是模型预测值和真实值之间的平均误差。
    - 如果 RMSE 很高， 说明模型的预测结果可能不太好。
##### 2.6.2 使用交叉验证进行更准确的评估
- 为什么要用交叉验证 (Cross-Validation)?
	- 因为评估只用训练集的一小部分数据， 可能会导致评估结果不准确。
	- 交叉验证可以将训练集分成 n 份 (比如 10 份)， 分别用其中 9 份进行训练， 用剩下 1 份 进行评估。 这样 就能得到 n 个 RMSE 结果， 更能全面地反映 模型的表现！

- 交叉验证的好处:
	- 更加准确 地测试了不同模型的效果.
	- 可以帮助 避免 overfitting (过度拟合， 模型在训练集上表现很好，但在新数据上表现很差)。
##### 2.6.3 尝试更多模型
- 除了 线性回归 和 决策树， 还有很多其他的模型 (比如 随机森林RandomForestRegressor) 
- 你应该 尝试 不同的 model, 看看 哪个 效果 更好。

保存模型:
- 为了方便比较不同模型，应该保存训练好的模型，方便你之后使用和再次评估。
- joblib.dump(my_model, "my_model.pkl") 可以保存 模型 到 一个 pkl 文件 。
#### 2.7 微调模型
已经拥有了几个候选的机器学习模型， 接下来就该对它们进行微调， 让它们变得更加 powerful！
##### 2.7.1 网格搜索
- 之前我们是手动调整模型的超参数 ，这非常耗时，而且可能会错过最佳的参数组合 。
- 网格搜索 (Grid Search) 可以自动尝试所有可能的参数组合，然后找到最好的一组
- 如何使用
	- 创建 param_grid 字典， 指定 你要 尝试 的 参数 和 值 。
	- 使用 GridSearchCV 这个 神奇 的 工具 ， 它会自动在所有组合上训练模型，并使用交叉验证方法进行评估 。
	- 最后它会告诉你哪种参数组合的效果最好！
##### 2.7.2 随机搜索
- 如果 参数 范围很大 ，网格搜索 需要 尝试 所有 组合 ，可能会很耗时。
- 随机搜索 (RandomizedSearchCV) 就 像 是在所有参数组合中 "随机 抽取" 几个 ， 进行训练和评估 ，它可以更快地找到一个不错的 参数 组合。
- 随机搜索的好处:
	- 避免了搜索所有组合，提高了效率 。
	- 可以控制搜索的迭代次数，可以根据时间预算来控制搜索范围 
##### 2.7.3 集成方法
- 将多个表现不错的模型组合起来，就可以获得一个性能更强大的模型 。
- 就像 随机森林 (RandomForestRegressor) ，是由很多个决策树组合起来的 ，它往往比单个决策树表现更好。
##### 2.7.4 分析错误
- 当你找到了一个表现不错的模型，你可以分析它的错误，看看是什么原因导致了这些 错误 。
- 例如，看看哪些属性有更大的影响，可以帮助你改进模型 。
##### 2.7.5 测试集评估
用 测试集 来对最终模型进行评估，这是 一个关键的步骤，它可以帮助你了解模型在 新的数据上的表现。
#### 2.8  启动、监控和维护你的系统
- 部署模型：
	- 使用 joblib 保存模型，便于在生产环境中读取模型。
	- 将模型封装成 Web 服务，提供 REST API 接口。
	- REST API 可以帮助模型更容易地被各种应用程序调用。
- 监控模型：
	- 模型上线后，需要持续监控它的表现，例如预测精度、响应时间等。
	- 模型会随着时间推移而 “腐烂”，原因可能是新的数据出现了变化，也可能是模型本身出现了问题。
	- 监控方式可以用人工评估，也可以用机器自动评估。书中还提到了使用验证码 (CAPTCHA) 来收集用户反馈。
- 维护模型：
	- 定期收集和标注新数据。
	- 定期重新训练模型，使用更新的数据来避免模型 “腐烂”。
	- 监控模型的输入数据。
- 备份和回滚：
	- 定期备份模型和数据，以便在发生问题时能及时恢复。
	- 建立模型更新和回滚机制，方便快速地进行版本控制。
#### 2.10 练习题
使用本章的房屋数据集完下面的练习题：

1.使用不同的超参数，如kernel="linear"（具有C超参数的多种值）或kernel="rbf"（C超参数和gamma超参数的多种值），尝试一个支持向量机回归器（sklearn.svm.SVR），不用担心现在不知道这些超参数的含义。最好的SVR预测器是如何工作的？

2.尝试用RandomizedSearchCV替换GridSearchCV。

3.尝试在准备流水线中添加一个转换器，从而只选出最重要的属性。

4.尝试创建一个覆盖完整的数据准备和最终预测的流水线。

5.使用GridSearchCV自动探索一些准备选项。
### 第三章 分类
本章开始介绍机器学习中的另一个重要任务：**分类**。不同于第二章的回归任务预测数值，分类任务则是预测样本所属的类别。
#### 3.1 MNIST
- 什么是MNIST数据集？
MNIST 数据集是一个非常经典的机器学习数据集，包含 70,000 张手写数字图片。每张图片都是 28 像素 x 28 像素 的黑白图像，并用其代表的数字进行标记（0 到 9）

- MNIST数据集的特点？
广泛使用: 作为机器学习领域的 “Hello World”， MNIST 被用来测试各种新分类算法的性能
易于理解: MNIST 数据集简单易懂，易于上手，适合入门学习机器学习。

- 如何获取MNIST数据集？
Scikit-Learn 提供了方便的工具来下载流行数据集，包括 MNIST。
```python
from sklearn.datasets import fetch_openml 
mnist = fetch_openml('mnist_784', version=1)

# fetch_openml 函数：从 OpenML 网站获取数据集。
# 'mnist_784' : MNIST 数据集的名称
# version=1 : 指定数据集的版本号
```

- MNIST数据集的结构？
Scikit-Learn 加载的数据集通常具有类似的字典结构，包括：
DESCR 键: 描述数据集，例如来源和用途。
data 键: 包含特征数据，是一个数组，每个样本对应一行，每个特征对应一列。
target 键: 包含标签数据，是一个数组，每个元素对应相应样本的标签类别。
![[Pasted image 20241229210430.png]]![[Pasted image 20241229210501.png]]
#### 3.2 训练二元分类器
![[Pasted image 20241229211906.png]]
这一节，我们开始实际训练一个分类器，它可以识别手写数字中的数字 5。
- <mark style="background: #FFB8EBA6;">步骤1：创建二元目标向量</mark>
首先我们要简化任务，只识别一个数字，例如数字 5。 为了实现这个，我们需要创建 二元目标向量，表示样本是或不是 5。
```python
y_train_5 = (y_train == 5) # True表示数字5，False表示不是5
y_test_5 = (y_test == 5) 
```

- <mark style="background: #FFB8EBA6;">步骤2：训练一个SGD分类器</mark>
接下来，我们需要选择一个分类器来完成识别任务。这里选择的是 <mark style="background: #ADCCFFA6;">随机梯度下降</mark>（SGD）分类器，它在处理大型数据集时表现出色。
```python
from sklearn.linear_model import SGDClassifier 

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(X_train, y_train_5)

# SGDClassifier : 随机梯度下降分类器。
# random_state=42 : 设置 随机种子， 保证训练结果可复现。
```

- 步骤 3： 使用训练好的分类器进行预测
现在，我们已经训练好了一个可以识别数字 5 的分类器，可以使用它来预测新的样本
```python
sgd_clf.predict([some_digit])
```

预测结果：array([ True])
表示分类器认为该图像代表数字 5。
#### 3.3 性能测量
##### 3.3.1 使用交叉验证测量准确率
- **什么是交叉验证？**  
    交叉验证是一种评估模型性能的方法，它将数据划分成多个折叠，然后循环使用不同的折叠进行训练和测试，最终根据多个折叠的测试结果来评估模型整体性能。
- **如何实现交叉验证？**
    - 使用 `cross_val_score()` 函数， 它 自动 执行 K 折交叉验证， 并 返回 每个 折叠 的 准确率 分数.
    - 通过 `StratifiedKFold()` 函数 实现 手工 交叉 验证 ， 代码 更 灵活。
- **准确率的局限性：**
    - 在不平衡数据集中，准确率可能无法真实反映模型的性能.
    - 例如：当一个类别占 90%，另一个类别只占10%，如果模型总是预测占90% 的那个类别。那么准确率还是90%，但实际上模型完全没有学习到另一个类别
##### 3.3.2 混淆矩阵
- 什么是混淆矩阵？
	- 它是一个表格，用于 显示 分类 模型 的 预测 结果 与 真实 类别 之间 的 关系。
![[Pasted image 20250101173545.png|400]]
- 混淆矩阵的元素：
	- **真正类(TP)**: 正确预测为正类的样本
	- **假正类(FP)**: 错误预测为正类的样本
	- **假负类(FN)**: 错误预测为负类的样本
	- **真负类(TN)**: 正确预测为负类的样本
- 混淆矩阵的用途 :
	- 通过混淆矩阵可以更加直观地了解模型在不同类别上的表现
	- 有助于分析模型的误差来源
- 如何获取混淆矩阵
	- 使用 confusion_matrix() 函数 ， 传入真实标签数据和预测标签数据
##### 3.3.3 精度和召回率
- **什么是 精度 (Precision) ?**
    - 预测为正类的样本中，真正正类的比例
    - 公式 : $\frac{TP}{(TP + FP)}$ 
    - 意义： 精度衡量的是模型预测为正类时，有多大把握是正确的

- **什么是 召回率 (Recall) ?**
    - 真正正类中，被模型正确预测为正类的比例
    - 公式:$\frac{TP}{(TP + FN)}$
    - 意义：召回率衡量的是模型能找到多少真正正类
    
- **F1 分数**
	- 定义：精度和召回率的调和平均数
	- 公式：$2 * ( precision * recall ) / (precision + recall )$
	- 意义：F1 分数综合考虑了精度和召回率，能够在一定程度上反映模型的整体性能。当精度和召回率都高时，F1分数也会很高
	- 用例：当需要平衡精度和召回率时，F1分数是一个常用的指标
##### 3.3.4 精度/召回率权衡
- 什么是 精度 / 召回率 权衡？
	- **精度（Precision）** 衡量的是<mark style="background: #ADCCFFA6;">模型预测为正类时，有多大把握是正确的</mark>。
	- **召回率（Recall）** 衡量的是<mark style="background: #ADCCFFA6;">模型能找到多少真正正类</mark>。
	- 在很多情况下，<mark style="background: #FFB8EBA6;">提高精度会降低召回率</mark>，反之亦然。这就是<mark style="background: #FFB8EBA6;">精度 / 召回率权衡。</mark>

- <mark style="background: #FF5582A6;">决策阈值</mark> 如何影响 精度 / 召回率 权衡？
	- 分类模型会计算一个分数，代表模型对样本属于正类的置信程度。
	- 决策阈值是一个<mark style="background: #FFB8EBA6;">临界值</mark>：
		- 当分数大于或等于阈值时，模型会将样本分类为正类。
		- 当分数小于阈值，模型会将样本分类为负类。

- <mark style="background: #FFB8EBA6;">调整 决策阈值 </mark>如何改变精度和召回率？
	- <mark style="background: #ADCCFFA6;">提高决策阈值</mark>
		- 模型会更严格地预测样本为正类，减少假正类 (FP)，<mark style="background: #BBFABBA6;">提高精度</mark>。
		- 模型会更容易将样本预测为负类，更多的真正正类 (TP) 被丢弃，<mark style="background: #BBFABBA6;">降低召回率。</mark>
		
	- <mark style="background: #ADCCFFA6;">降低决策阈值</mark>
		- 模型会更容易地预测样本为正类，找到更多的真正正类 (TP)，<mark style="background: #BBFABBA6;">提高召回率。</mark>
		- 模型会更容易将样本预测为正类，更多的假正类 (FP) 被误判，<mark style="background: #BBFABBA6;">降低精度.</mark>
![[Pasted image 20241230185653.png]]

- 如何<mark style="background: #FFB8EBA6;">选择合适的决策阈值</mark>？
	- 取决于你的任务：
		- 如果你更<mark style="background: #BBFABBA6;">重视准确性</mark>，则需要提高阈值，降低召回率，但确保预测结果的可靠性。
		- 如果你更<mark style="background: #BBFABBA6;">重视找到所有的目标</mark>，则需要降低阈值，提高召回率，即使可能有更多误判。
	- 工具：
		- 可以使用 `precision_recall_curve()` 函数绘制精度和召回率关于阈值的曲线图，以帮助你选择合适的决策阈值。
##### 3.3.5 ROC曲线
- ROC(receiver operating characteristic)曲线是什么？
	- <mark style="background: #FFB8EBA6;">ROC 曲线（受试者工作特征曲线）是一种常用的二元分类模型性能评测工具</mark>
	- 它绘制了真正类率 (TPR) 和假正类率 (FPR) 之间的关系。

- ROC曲线的解释：
	- 横坐标 (FPR)：假正类率，表示模型将<mark style="background: #ADCCFFA6;">负类样本误判为正类的比例</mark>。
	- 纵坐标 (TPR)：真正类率，表示模型<mark style="background: #ADCCFFA6;">正确识别出正类样本</mark>的比例。
	- ROC 曲线的位置：
		- 模型的 ROC 曲线越靠近左上角，模型的性能越好。
		- 一个完美的模型的 ROC 曲线会经过 (0, 1) 点，即模型能够完全识别正类样本，且不会误判负类样本。
		- 随机模型的 ROC 曲线是一条对角线，此时正类样本和负类样本的预测结果是随机的。
![[Pasted image 20241230190936.png]]

- ROC曲线下面积（AUC）
	- <mark style="background: #FFB8EBA6;">AUC 是 ROC 曲线下面的面积</mark>，它是一种更简洁地衡量模型性能的指标。
	- AUC 介于 0 和 1 之间：
		- AUC 越高，表示模型越好，区分正类样本和负类样本的能力越强。
		- 一个完美的模型的 AUC 为 1。
		- 随机模型的 AUC 为 0.5。


- ROC曲线 和 PR( 精度 / 召回率 ) 曲线
	- 当正类很少见或者你更关注假正类时，使用 PR 曲线。
	- 当正类很常见或者你更关注假负类时，使用 ROC 曲线。

- 如何绘制ROC曲线？
	- 使用 `roc_curve()` 函数，传入真实标签数据和预测得分数据。
	- 使用 `plot_roc_curve` 函数绘制ROC 曲线。
#### 3.4 多类分类器
- **多类分类器** 可以将样本分为多个类别，而 **二类分类器** 只能分为两个类别。
- **处理多类分类问题的方法：**
	- <mark style="background: #FFB8EBA6;">一对多 (OvR)/ 一对剩余 (One-vs-Rest)</mark>： 训练多个二元分类器，每个分类器对应一个类别，识别该类别与其他所有类别的差异。例如，对于 10 类，训练 10 个分类器，分别识别数字 0 到数字 9。
	- <mark style="background: #FFB8EBA6;">一对一 (OvO/One-vs-One)</mark>: 训练每两个类别之间的二元分类器。例如，对于 10 类，训练 45 个分类器，分别区分 0 和 1， 0 和 2， 1 和 2...

- Scikit-learn 的处理：
	- Scikit-learn 可以自动根据所用分类器选择合适的策略，默认使用 OvR 或 OvO。
	- 例如，使用 SVC 进行多类分类时，Scikit-learn 会自动训练多个二元分类器，并根据它们的分数选择最佳类别。
#### 3.5 误差分析
- **目的**：找到模型犯错的模式，以改进模型性能
- 方法：
	- **混淆矩阵分析**：
		- `confusion_matrix()` 函数可以查看<mark style="background: #ADCCFFA6;">模型预测结果与真实标签之间的关系。</mark>
		- 通过<mark style="background: #BBFABBA6;">分析矩阵对角线 (正确预测) 和非对角线 (错误预测) 的分布</mark>，可以找到模型容易混淆的类别。
		- 可以对<mark style="background: #BBFABBA6;">混淆矩阵进行归一化，除以每行 (实际类别) 的总和，来查看每个类别上的错误率</mark>。
	- **查看单个错误样本**：
		- 可以查看模型错误预测的样本，并分析它们的特点，例如，错误的样本是否有一些共同的特征？这些特征是否可以作为改进模型的参考？
- 示例：
	- 使用 `SGDClassifier` 对 MNIST 数据集进行分类:
		- 分析混淆矩阵矩阵，发现数字 8 被误判的频率较高，而 3 和 5 容易被混淆。
		- 分析错误分类的样本，发现 3 和 5 之间的主要差异在于连接顶线和下方弧线的中间那段小线条的位置。
		- 结论：模型对图像的移位和旋转比较敏感，可以尝试对图片进行预处理，确保它们位于中心位置，并且没有旋转。
#### 3.6 多标签分类
- 多标签分类：每个样本可能被分配多个标签。
	- 例如人脸识别：一张照片可能包含多个人脸；
	- 图像分类：一张图片可能包含多个物体类别。
- 在实际应用中，多标签分类器 可以用于：
	- 图片标注：识别一张图片中所有的物体。
	- 文档分类：标定一篇文档中所有的主题。
- 示例：
	- 创建一个 y_multilabel 数组，包含两个标签：
		- 第一个标签表示数字是否大数 (大于等于 7)
		- 第二个标签表示数字是否奇数。
	- 使用 `KNeighborsClassifier` 训练一个多标签分类器：
	- 预测结果包含两个标签：
```python
y_train_large = (y_train >= 7) 
y_train_odd = (y_train % 2 == 1) 
y_multilabel = np.c_[y_train_large, y_train_odd]

# 使用 KNeighborsClassifier 训练一个多标签分类器：
knn_clf = KNeighborsClassifier() 
knn_clf.fit(X_train, y_multilabel)

# 预测结果包含两个标签：
knn_clf.predict([some_digit]) 
# 例如，输出 [[False, True]]，代表不是大数，但是奇数
```
- 评估多标签分类器:
	- 有多种评估方法，选择合适的方法取决于你的项目
	- 可以使用 f1_score 计算每个标签的F1分数，并计算平均分数。
	- 如果不同标签的重要性不同，可以使用权重平均 average="weighted" 来计算平均分数。
```python
# 使用 cross_val_predict 进行预测：
y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)

# 使用 f1_score 计算 平均F1分数：
f1_score(y_multilabel, y_train_knn_pred, average="macro") # 所有标签权重相同 

f1_score(y_multilabel, y_train_knn_pred, average="weighted") # 按照标签支持度调整权重
```
#### 3.7 多输出分类
- **多输出分类** 是 **多标签分类** 的 泛化
    - 不仅可以为每个样本分配多个标签。
    - 还可以为每个标签指定多个值（不仅是二元值）。
- 例如：
    - 去除 图片 中的 噪声。
    - 每个像素点对应一个标签，每个标签可以是0到255之间的任意值（像素强度）
- 代码示例
	- 创建有噪声的训练集和测试集
	- 使用`KNeighborsClassifier`训练模型
	- 预测去除噪声后的图片
```python
# 1.创建有噪声的训练集和测试集
# 为 MNIST 图片添加噪声 
noise = np.random.randint(0, 100, (len(X_train), 784)) X_train_mod = X_train + noise 
noise = np.random.randint(0, 100, (len(X_test), 784)) 
X_test_mod = X_test + noise 

# 设定目标数据为原始图片 
y_train_mod = X_train 
y_test_mod = X_test

# 2.使用KNN训练模型
knn_clf.fit(X_train_mod, y_train_mod)

# 3.预测去除噪声后的图片
clean_digit = knn_clf.predict([X_test_mod[some_index]])
```
#### 3.8 练习题
1.为MNIST数据集构建一个分类器，并在测试集上达成超过97%的准确率。提示：KNeighborsClassifier对这个任务非常有效，你只需要找到合适的超参数值即可（试试对weights和n_neighbors这两个超参数进行网格搜索）。

2.写一个可以将MNIST图片向任意方向（上、下、左、右）移动一个像素的功能。然后对训练集中的每张图片，创建四个位移后的副本（每个方向一个），添加到训练集。最后，在这个扩展过的训练集上训练模型，测量其在测试集上的准确率。你应该能注意到，模型的表现甚至变得更好了！这种人工扩展训练集的技术称为数据增广或训练集扩展。

3.Kaggle上非常棒的起点：处理泰坦尼克（Titanic）数据集。

4.创建一个垃圾邮件分类器（更具挑战性的练习）：
- 从Apache SpamAssassin的公共数据集中下载垃圾邮件和非垃圾邮件。
- 解压数据集并熟悉数据格式。
- 将数据集分为训练集和测试集。
- 写一个数据准备的流水线将每封邮件转换为特征向量。你的流水线应将电子邮件转换为一个“指示出所有可能的词存在与否”的（稀疏）向量。比如，如果所有的邮件都只包含四个词“Hello”“how”“are”“you”，那么邮件“Hello you Hello Hello you”会被转换成为向量[1，0，0，1]（意思是“Hello”存在，“how”不存在，“are”不存在，“you”存在），如果你希望算上每个词出现的次数，那么这个向量就是[3，0，0，2]。
- 在流水线上添加超参数来控制是否剥离电子邮件标题，是否将每封邮件转换为小写，是否删除标点符号，是否将“URLs”替换成“URL”，是否将所有小写number替换为“NUMBER”，甚至是否执行词干提取（即去掉单词后缀，有可用的Python库可以实现该操作）。
- 最后，多试几个分类器，看看是否能创建出一个高召回率且高精度的垃圾邮件分类器。

### 第四章 训练模型
- 四个关键部分:
	-   **线性回归**
		- 两种训练方法:
			- <mark style="background: #FFB8EBA6;">闭式方程方法</mark>: 直接计算最优参数 (即最小化成本函数的参数）。
			- <mark style="background: #FFB8EBA6;">梯度下降方法</mark>: 迭代调整参数，最终收敛到与闭式方程方法相同的结果。
	- **多项式回归**
		- 更复杂的模型，适合非线性数据集。
		- 如何避免模型过拟合 (过度匹配训练数据)
		- 使用 <mark style="background: #FFB8EBA6;">学习曲线</mark> 诊断过拟合。
		- 使用 <mark style="background: #FFB8EBA6;">正则化技巧</mark> 减少过拟合。
	- **Logistic 回归**
		- 常用分类模型，用于二元分类（例如，区分 0 和 非 0）。
	- **Softmax 回归**
		- 用于多类分类（例如，区分数字 0 到 9）。
- 重要知识点:
	- **梯度下降：**
		- 一种迭代学习方法，通过逐渐调整参数，降低成本函数的值。
		- 有几种变体：
			- <mark style="background: #FFB8EBA6;">批量梯度下降</mark>: 使用所有训练样本计算梯度。
			- <mark style="background: #FFB8EBA6;">小批量梯度下降</mark>: 使用一小部分训练样本计算梯度。
			- <mark style="background: #FFB8EBA6;">随机梯度下降</mark>: 使用单个训练样本计算梯度。
	- **学习曲线：**
		- 用于判断模型是否过拟合的工具。
		- 通过观察模型在训练集和测试集上的性能变化，可以找出问题的所在。
	- **正则化技巧:**
		- 用于减少过拟合的方法
		- 通过限制模型的复杂度，防止模型过度拟合训练数据。
#### 4.1 线性回归
- 模型介绍
	- 基本概念：线性回归模型利用输入特征的线性组合 (加权求和) 来预测输出值。
	- 示例
		- 第 1 章中提到的生活满意度模型 $life_satisfaction=θ_0+θ_1×GDP_per_capita$ 就是一个线性回归模型，其中 $GDP_per_capita$ 是输入特征，$θ_0$ 和 $θ_1$ 是模型参数。
	- 公式
		- $h_{\theta}(x)={\theta}_0+{\theta}_1x_1+{\theta}_2x_2+...+{\theta}_nx_n$
		- 向量化形式：$h_{\theta}(x)={\theta}^{T}x$
			- $\theta$：模型的参数向量，包含偏差项$\theta_0$和特征权重$\theta_1$到$\theta_n$
			- $x$：实例的特征向量，包含从$x_0$到$x_n$，其中$x_0$始终为1
	- 训练目标：找到最佳模型参数 ${\theta}$ 使得<mark style="background: #FFB8EBA6;">模型在训练数据集上的均方误差（MSE）最小</mark>

- 训练模型
	- 测量模型性能：均方根误差RMSE和均方误差MSE是常用的性能指标
	- 目标：找到使得MSE最小的$\theta$值
	- 方法：<mark style="background: #ABF7F7A6;">闭式解（closed-form solution）</mark>
		- 1. <mark style="background: #FFB8EBA6;">标准方程（Normall Equation）</mark>
			- $\theta={(X_{T}X)}^{-1}X^{T}y$
			- X：训练数据集特征矩阵
			- y：训练数据集目标值向量
		- 2.奇异值分解SVD：当矩阵$X^{T}X$不可逆时，可以用SVD方法求解$\theta$
		- 3.其他方法（如：梯度下降）：当样本量很大时，标准方程和SVD方法可能会很慢，这时候可以考虑使用梯度下降等更适合大规模数据的训练方法
- 代码示例
```python
# 1.使用标准方程训练：
X_b = np.c_[np.ones((100, 1)), X] # 添加 x0 = 1 到每个实例 
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

# 2.使用sklearn库训练
from sklearn.linear_model import LinearRegression 
lin_reg = LinearRegression() 
lin_reg.fit(X, y)

# 3.使用scipy.linalg.lstsq训练
theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
```
- 计算复杂度
	- **标准方程:** 计算 ${X^{T}X}^{-1}$ 的时间复杂度通常为 O ($n^{2.4}$) 到 O ($n^3$) 之间，其中 n 是特征数量。
	- **SVD 方法:** SVD 方法的时间复杂度约为 O ($n^2$)。
	- **训练时间:** 标准方程和 SVD 方法的复杂度都与样本数量呈线性关系。
	- **预测时间:** 模型训练好后，预测的时间复杂度与实例数量和特征数量都呈线性关系。
#### 4.2 梯度下降
- 核心思想：梯度下降是一种通用的优化算法，它能够为各种问题找到最优解。它的基本思想是：<mark style="background: #FFB8EBA6;">迭代地调整参数（比如模型的权重），使成本函数最小化。</mark>
- 具体步骤
	- **随机初始化**：首先，你随机选择一组初始的参数值 θ (就像你可能在一个随机的地方开始下山)。
	- **逐步调整**: 每次你都尝试沿着下降方向调整参数 θ，以减小成本函数的值 (就像是根据坡度选择下山的方向)。
	- **收敛**: 随着迭代次数的增加，参数 θ 会逐渐逼近成本函数的最小值 (就像你最终会抵达山谷的最底部)。
- 学习率
	- 学习率 η 控制了每次迭代中调整参数 θ 的步长。
		- 学习率过低，就像下山时步子太小，导致算法收敛速度很慢，需要很长时间才能找到最小值。
		- 学习率过高，就像下山时步子太大，可能直接越过最低点，甚至可能越走越远，导致算法发散，无法找到最优解。
- 挑战
	- 并不是所有的成本函数都是 “碗状” 的，可能存在各种复杂地形：
		- **局部最小值**: 成本函数可能有多个局部最小值，就像山谷里可能有多个凹陷，算法可能被困在其中一个凹陷中，而不是到达真正的最低点。
		- **平坦区域**: 当成本函数在局部区域非常平坦时，即使知道应该往下走，也很难找到往下走的明确方向，导致算法收敛非常慢
- 线性回归模型的MSE成本函数
	- 幸运的是，线性回归模型的均方误差 (MSE) 成本函数是一个凸函数，这意味着它只有一个全局最小值，没有局部最小值。而且 MSE 还是一个连续函数，它的斜率不会突然发生变化。
	- 这意味着，对于线性回归模型，只要你选择合适的学习率，并执行算法足够长时间，梯度下降最终是可以找到全局最小值的。
- 特征缩放
	- 虽然线性回归模型的 MSE 成本函数是一个 “碗状”，但如果模型的特征尺度差别很大，那么这个 “碗” 可能会变得非常细长。这会导致梯度下降算法收敛速度非常慢，就像在一个狭长的山谷里行走，需要很长时间才能到达谷底。
	- 为了解决这个问题，你需要对特征进行缩放，使所有特征值的大小比例大致相同。常用的特征缩放技术有：
		- 标准化 (Standardization)
		- 归一化 (Normalization)
##### 4.2.1 批量梯度下降 Batch Gradient Descent
- 核心思想：每次更新时，使用所有训练样本来计算梯度
- 优点：收敛过程比较稳定，能找到一个比较准确的最小值
- 缺点：当训练数据量很大时，每次迭代都需要处理所有数据，计算量非常大，速度很慢，尤其是在内存有限的情况下
- 例子：
	- 假设你想根据房子的面积 (x) 预测房子的价格 (y)。 你获得了 100 个房子面积和价格的数据样本。
	- 梯度下降过程：
		- 随机初始化参数 θ， 例如: θ = [0.5, -0.1] ( θ 对应模型参数， 决定了线性回归的斜率和截距)
		- 计算当前参数 θ 下，所有 100 个样本的成本函数（比如均方误差 MSE）。
		- 计算每个参数 θ 的偏导数 (梯度)， 并更新参数 θ。
		- 重复步骤 2-3，直到参数 θ 基本不再变化。
- 批量梯度下降 适合小规模数据集和内存有限的情况，更稳定但速度慢。
```python
eta = 0.1 # 学习率 
n_iterations = 1000 # 迭代次数 
m = 100 # 样本数量 

theta = np.random.randn(2, 1) # 随机初始化参数 

for iteration in range(n_iterations): 
	gradients = 2 / m * X_b.T.dot(X_b.dot(theta) - y) # 计算梯度
	theta = theta - eta * gradients # 更新参数
```
##### 4.2.2 随机梯度下降 Stochastic Gradient Desceny,SGD
- 核心思想： 每次迭代时，随机梯度下降 随机选择一个训练样本 ， 只基于这个样本计算梯度。
- 优点:
	- 由于每次只处理一个样本，计算量大幅减少，速度快，非常适合大规模数据集训练。
	- 由于其随机性，SGD 更容易跳出局部最优，更容易找到全局最小值。
- 缺点：由于一次只处理一个样本， 收敛过程不太稳定。
- 例子：
	- 假设你有一百万个房子的面积和价格数据。
	- 梯度下降过程:
		- 随机初始化参数 θ。
		- 每次迭代时，随机选择一个样本，并计算这个样本对应的预测价格与真实价格的误差。
		- 通过这个误差，计算参数 θ 的梯度。
		- 根据梯度更新参数 θ。
		- 重复步骤 2-4。
- 总结： 随机梯度下降在处理大规模数据集时非常有效，可以提高训练速度，但需要谨慎设置学习率调度，以平衡收敛速度和稳定性。
```python
n_epochs = 50 # 迭代次数 (epochs) 
t0, t1 = 5, 50 # 学习率调度参数 

def learning_schedule(t): 
	return t0 /(t + t1) # 学习率调度函数 

theta = np.random.randn(2,1) # 随机初始化参数 

for epoch in range(n_epochs): 
	for i in range(m): # 遍历训练集 
		random_index = np.random.randint(m) # 随机选择样本索引 
		xi = X_b[random_index:random_index+1] # 获取随机样本特征
		yi = y[random_index:random_index+1] # 获取随机样本标签
		gradients = 2 * xi.T.dot(xi.dot(theta) - yi) # 计算梯度
		eta = learning_schedule(epoch * m + i) # 学习率 
		theta = theta - eta * gradients # 更新参数
```
##### 4.2.3 小批量梯度下降 Mini-Batch Gradient Descent
- 核心思想: 每次迭代时，小批量梯度下降 从训练集中随机选取一小部分样本 （称为 “小批量”），然后使用这个小批量数据来计算梯度。
- 优点：
	- 速度比批量梯度下降快，因为它只处理一小部分数据
	- 收敛过程比随机梯度下降更加稳定，因为它还是利用了一小部分样本的信息，可以更准确地估计方向。
- 缺点：
	- 小批量梯度下降可能会遇到局部最小值，并且难以像随机梯度下降那样容易跳出 (不过，通过使用合适的学习率调度和优化策略可以一定程度上解决此类问题)。
- 解释： 小批量梯度下降可以看作是批量梯度下降和随机梯度下降之间的折衷方案， 它结合了两种方法的优点，并且在实践中被广泛应用。
- 小批量梯度下降 速度和稳定性相比随机梯度下降都有优势，是目前工业界使用频率较高的梯度下降方法。
![[Pasted image 20250107165902.png]]![[Pasted image 20250107165913.png]]![[Pasted image 20250107165924.png]]![[Pasted image 20250107165947.png]]
#### 4.3 多项式回归
1. **问题：非线性数据**
如果你的<mark style="background: #FFB8EBA6;">数据无法用一条简单的直线来拟合， 比如数据呈现出曲线形的趋势</mark>， 就需要使用非线性模型来进行拟合。

2. **解决方法：多项式回归**
多项式回归是一种使用线性模型来拟合非线性数据的方法。 核心思想是将每个特征的幂次方添加为一个新的特征， 例如将 x 扩展为 x^2、x^3 等等， 然后在这些扩展后的特征上训练线性模型。

3. **数学原理**
- 线性回归: 最简单的线性模型， 可以理解为一条直线， 模型方程为 $y = \theta_0 + \theta_1 * x$
- 多项式回归: 本质上也是线性模型，但它通过在特征上添加多项式项 (特征的幂次方) 来拟合曲线。
	- 一个简单的二次多项式可以表示成 $y = \theta_0 + \theta_1 * x + \theta_2 * x^2$
	- 多项式回归依然是线性模型， 因为它的参数 $\theta_i$ 仍然是线性的， 只不过是对原始特征进行了一些非线性变换 (加了幂次)。

- 例子：假设你收集了一些房子的面积 x (平方米) 和价格 y (万元) 的数据，发现数据呈现出一种曲线性趋势， 不像线性回归那样简单。
- 以下是用 sklearn.preprocessing. PolynomialFeatures 来处理数据和训练线性模型：
```python
import numpy as np 
from sklearn.preprocessing import PolynomialFeatures 
from sklearn.linear_model import LinearRegression 
import matplotlib.pyplot as plt 

# 生成数据 
m = 100 
X = 6 * np.random.rand(m, 1) - 3 
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1) 

# 扩展数据：添加平方项 (x^2) 
poly_features = PolynomialFeatures(degree=2, include_bias=False) X_poly = poly_features.fit_transform(X) 

# 使用扩展后的特征 X_poly 训练线性回归模型 
lin_reg = LinearRegression() 
in_reg.fit(X_poly, y) 

# 打印结果 
print("截距: ", lin_reg.intercept_) 
print("系数: ", lin_reg.coef_) 

# 预测 
y_pred = lin_reg.predict(X_poly) 

# 可视化结果 
plt.scatter(X, y, label="数据点") 
plt.plot(X, y_pred, color="r", label="多项式回归模型") plt.xlabel("房屋面积 (x)") 
plt.ylabel("房屋价格 (y)") 
plt.title("多项式回归示例") 
plt.legend() 
plt.show()
```
#### 4.4 学习曲线
- 1. **问题： 过拟合和欠拟合**
	- 过拟合 (Overfitting): 模型过度地学习了训练数据，导致对训练数据预测效果很好，但对新数据 (测试数据) 预测效果很差。
	- 欠拟合 (Underfitting): 模型没有捕捉到训练数据中的复杂模式，导致对训练数据和测试数据预测效果都不好。

- 2. **解决问题： 学习曲线**
	- **学习曲线**: 绘制的是<mark style="background: #FFB8EBA6;">模型在训练集和验证集上的性能指标随训练集大小（或训练迭代次数）的变化情况。</mark>
	- **观察学习曲线**: 能够帮助我们<mark style="background: #ABF7F7A6;">判断模型是过拟合还是欠拟合</mark>。

- 3. **解释学习曲线**
	- **步骤**：
		- 将训练数据分成训练集 (train set) 和验证集 (validation set)。
		- 使用不同大小的训练子集 (从很小开始，逐渐增大) 去训练模型。
		- 在每个训练子集上训练模型后， 计算模型在训练子集和验证集上的性能 (比如 MSE)
		- 将训练子集的大小作为横坐标， 将性能指标作为纵坐标， 绘制两条曲线， 这两条曲线就是学习曲线。
	- **学习曲线的模式：**
		- <mark style="background: #FF5582A6;">欠拟合:</mark>
			- <mark style="background: #ADCCFFA6;"> 训练集和验证集上的性能都比较差</mark>， 并且随着训练集大小的增加， 两条曲线会逐渐趋于平稳， 并且两条曲线距离很近。
			- 原因： 模型过于简单 (例如， 線性回归模型) 无法学习到训练数据中的复杂模式。
			- 解决方法： 使用更<mark style="background: #ADCCFFA6;">复杂的模型 (例如，多项式回归) </mark>、 添加更多特征 (features) 或 改进特征
		- <mark style="background: #FF5582A6;">过拟合</mark>:
			- <mark style="background: #ADCCFFA6;">训练集上的性能很好， 但是验证集上的性能很差</mark>， 并且随着训练集大小的增加， 训练集上的性能继续提升， 而验证集上的性能基本保持不变。
			- 原因： 模型过于复杂 (例如， 高阶多项式回归) 过度地学习了训练数据中的噪声 (noise)。
			- 解决方法: 使用<mark style="background: #ABF7F7A6;">正则化 (Regularization) 技术 (例如 L1/L2 正则化)</mark> 来降低模型的复杂度， 增加更多的训练数据。
		- <mark style="background: #FF5582A6;">最佳情况</mark>:
			- 随着训练集大小的增加， 训练集和验证集上的性能都逐渐提升， 并且最终两条曲线逐渐趋于接近。
			- 原因: 模型的复杂度恰到好处。
			- 解决方法: 继续增加训练数据。

- 4. **偏差 - 方差权衡 (Bias-Variance Tradeoff)**
	- <mark style="background: #ADCCFFA6;">偏差 (Bias): 模型的预测结果与真实值的平均差距</mark>。
		- 高偏差: 模型过于简单 (例如线性模型)， 无法很好地拟合训练数据。
		- 低偏差: 模型过于复杂 (例如高阶多项式模型)， 能够很好地拟合训练数据，但也容易过拟合。

	- <mark style="background: #ADCCFFA6;">方差 (Variance): 模型在不同训练集上的预测结果的变化程度。</mark>
		- <mark style="background: #ABF7F7A6;">高方差: 模型过于复杂， 对训练数据的微小变化很敏感， 容易过拟合。</mark>
		- 低方差: 模型结构简单， 对训练数据的变化不敏感， 容易欠拟合。

	- 关系: <mark style="background: #FFB8EBA6;">偏差和方差是一个权衡 (tradeoff)</mark>:
		- 提高模型的复杂度通常会降低偏差， 但可能会提高方差。
		- 降低模型复杂度则可能会降低方差， 但可能会提高偏差。
```python
from sklearn.metrics import mean_squared_error 
from sklearn.model_selection import train_test_split 
import matplotlib.pyplot as plt 

# 生成数据 
m = 100 
X = 6 * np.random.rand(m, 1) - 3 
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1) 

# 定义函数：绘制学习曲线 
def plot_learning_curves(model, X, y): 
	X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2) 
	train_errors, val_errors = [], [] 
	
	for m in range(1, len(X_train) + 1): # 遍历训练集的大小 (1-100) 
		model.fit(X_train[:m], y_train[:m]) 
		y_train_predict = model.predict(X_train[:m])
		y_val_predict = model.predict(X_val)
		
		train_errors.append(mean_squared_error(y_train[:m], y_train_predict)) 
		val_errors.append(mean_squared_error(y_val, y_val_predict)) 
		
	plt.plot(np.sqrt(train_errors), "r-+", linewidth=2, label="训练集") 
	plt.plot(np.sqrt(val_errors), "b-", linewidth=3, label="验证集") 
	plt.xlabel("训练集大小") 
	plt.ylabel("均方误差 (RMSE)") 
	plt.title("学习曲线") 
	plt.legend() 
	plt.show() 
	
# 绘制线性回归模型学习曲线 
from sklearn.linear_model import LinearRegression 
lin_reg = LinearRegression() 
plot_learning_curves(lin_reg, X, y) 

# 绘制 10 阶多项式模型学习曲线 
from sklearn.pipeline import Pipeline 
	polynomial_regression = Pipeline([ 					 		("poly_features",PolynomialFeatures(degree=10,include_bias=False)), ("lin_reg", LinearRegression()), 
									 ]) plot_learning_curves(polynomial_regression, X, y)
```
#### 4.5 正则化线性模型
- 1. **问题：过拟合 (Overfitting)**
	- 训练模型就像训练一个学生， 如果学生只学习了考试题， 那么他可能会在试卷中取得好成绩， 但如果遇到一些新的问题， 他可能就无能为力了。 这就是过拟合带来的问题。

- 2. **解决问题：正则化**
	- **正则化** 就好比给学生布置一些额外的作业， 让学生能够学到更基础、更通用、更扎实的知识。
	- **正则化的核心思想**： <mark style="background: #ADCCFFA6;">通过限制或约束模型的复杂度来防止过拟合。</mark>
	- 方法: 在模型训练时， <mark style="background: #FFB8EBA6;">添加一些额外的 “惩罚项” （正则项） 到原来的成本函数中</mark>， 这种惩罚项通常是通过<mark style="background: #ABF7F7A6;">调整 (限制) 模型参数的大小 (通常是权重) </mark>来实现的。

- 3. 常见的正则化线性模型
##### 4.5.1 岭回归 Ridge Regression
- 算法原理
	- 在成本函数中加入一个正则化项， 这个正则化项是所有参数 θ (除了 $θ_0$ ， $θ_0$ 一般不进行正则化) 的平方和， 乘以一个系数 α。
- 核心公式
	- 岭回归的成本函数 (Cost Function)
$$J(θ) = \frac{1}{2m} \sum_{i=1}^{m} [h(θ, x^{(i)}) - y^{(i)}]^2 + α \frac{1}{2} \sum_{j=1}^{n} θ_j^2$$
		- $h(θ, x^{(i)})$ 是模型对第 i 个样本的预测值。
		- $y^{(i)}$ 是第 i 个样本的真实值。
		- $m$ 是训练样本的数量。
		- $α$ 是正则化系数， 控制着正则化项的强度。

- 参数更新: 使用梯度下降来更新参数 θ， 方法与没有正则化的情况类似， 只是在计算梯度时增加了正则化项的梯度。
- 优点:
	- 岭回归可以<mark style="background: #ADCCFFA6;">降低模型的方差</mark>， 从而提高模型的泛化能力， 减少过拟合。
- 缺点:
	- 岭回归不会像 Lasso 回归那样直接将无关的特征的权重设置为 0， 它只是让权重变小， 因此， <mark style="background: #FFB8EBA6;">岭回归并不具备特征选择的功能</mark>
##### 4.5.2 Lasso回归 Least Absolute Shrinkage and Selection Operator Regression
- 算法原理:
	- 在成本函数中加入一个正则项， 这个正则项是所有参数 θ 的绝对值之和， 乘以一个系数 α。
- 核心公式:
	- Lasso 回归的成本函数 (Cost Function)
$$J(θ) = \frac{1}{2m} \sum_{i=1}^{m} [h(θ, x^{(i)}) - y^{(i)}]^2 + α \sum_{j=1}^{n} |θ_j|$$
- 优点:
	- Lasso 回归可以<mark style="background: #ADCCFFA6;">降低模型的方差</mark>， 提高模型的泛化能力，减少过拟合。
	- Lasso 回归可以进行<mark style="background: #ADCCFFA6;">自动特征选择， 它会把一些不重要的特征的权重直接设定为 0， 实现特征选择。</mark>
- 缺点:
	- 当<mark style="background: #FFB8EBA6;">特征数量很大， 或特征之间强相关</mark>时， Lasso 会比较不稳定。
##### 4.5.3 弹性网络 Elastic Net
- 算法原理:
	- 综合岭回归和 Lasso 回归的优势， 使用它们的正则项的组合。
- 核心公式:
	- 弹性网络的成本函数 (Cost Function)
$$J(θ) = \frac {1} {2m} \sum_{i=1}^{m} [h(θ, x^{(i)}) - y^{(i)}]^2 + α (r \sum_{j=1}^{n} |θ_j| + \frac{1-r}{2} \sum_{j=1}^{n} θ_j^2)$$
	- r 是一个介于 0 到 1 之间的参数， 控制着岭回归和 Lasso 回归的权重之比。
	- 当 r = 0 时， 弹性网络退化为 岭回归。
	- 当 r = 1 时， 弹性网络退化为 Lasso 回归。
- 优点:
	- 兼具了岭回归和 Lasso 回归的优点， 可以降低模型的方差， 提高泛化能力， 同时可以进行特征选择
- 缺点
	- 需要调整两个参数 α 和 r
##### 4.5.4 提前停止 Early Stopping
- 算法原理：
	- 在梯度下降的训练过程中， 监控验证集上的误差。
	- 当验证集上的误差不再下降， 甚至开始上升 (模型开始过拟合)， 就停止训练。
- 优点:
	- 简单易行， 有效地防止过拟合。
- 缺点:
	- 需要一个验证集， 可能会浪费一部分训练数据.
#### 4.6 逻辑回归
- 问题：二元分类
	- 许多问题需要将数据分成两类， 例如：邮件是垃圾邮件还是正常邮件？图片是猫还是狗？肿瘤是良性还是恶性？

- 解決方案：逻辑回归
	- <mark style="background: #FFB8EBA6;">逻辑回归 (Logistic Regression) 是一个用来解决二元分类问题的线性模型</mark> (虽然名字中有 “回归”，但它实际上是一个分类模型)。
	- 核心思想: 逻辑回归并不是直接预测结果， 而是<mark style="background: #ABF7F7A6;">预测某个样本属于某个类别的概率。</mark>
	- 判别规则: <mark style="background: #ADCCFFA6;">如果预测的概率大于 0.5， 就将其归类为 "正类" (1)；否则，就归类为 "负类" (0)。</mark>

- 数学原理
	- 线性模型: <mark style="background: #ADCCFFA6;">逻辑回归模型是建立在线性模型的基础上的</mark>。 与线性回归模型类似， 它也会计算输入特征的加权和 (加上偏置项)， 但它不是直接输出结果， 而是将结果代入到一个叫做 sigmoid 函数(也叫作 逻辑函数) 的函数中。
	- **sigmoid 函数**:<mark style="background: #ABF7F7A6;"> sigmoid 函数将一个数值映射到 0 到 1 之间</mark>， 这个映射结果可以理解为该样本属于正类的概率。
		- 公式 (4-14): $σ(t) = \frac{1}{1 + e^{-t}}$， 其中 t 是线性模型计算的输入特征的加权和 (加上偏置项)。
	- 概率估计: 逻辑回归模型的预测概率：
		- 公式 (4-13): $P(y=1|x) = σ(θ^T x)$， 其中 θ 是模型参数， x 是样本特征， $θ^T x$ 是<mark style="background: #FF5582A6;">输入特征的加权和</mark>， σ 是 sigmoid 函数。
	- 决策边界: 由于 sigmoid 函数的特性， 当 $θ^T x > 0$时， $σ(θ^T x) > 0.5$， 模型预测为正类 (1)； 当 $θ^T x < 0$ 时， $σ(θ^T x) < 0.5$， 模型预测为负类 (0)。
		- 所以 $θ^T x = 0$ 定义了一条直线， 我们将这条直线称为 <mark style="background: #FF5582A6;">决策边界</mark>。 该直线将样本空间分成了两部分， 一边对应的预测值为 1， 一边对应的预测值为 0。
	- 成本函数 (Cost function): 逻辑回归模型的成本函数 通常使用 <mark style="background: #ABF7F7A6;">对数损失函数 (Log loss) 或者交叉熵函数。</mark> 这种成本函数的特性是， <mark style="background: #CACFD9A6;">当模型对正类实例预测的概率接近 1， 对负类实例预测的概率接近 0 时， 成本函数的值最小</mark>， 这符合我们的预期， 说明模型预测效果比较好。
		- 公式 (4-16): 单个训练实例的成本函数 $$J(θ, x^{(i)}, y^{(i)}) = - y^{(i)} \log(h(θ, x^(i))) - (1-y^{(i)}) \log(1-h(θ, x^(i))$$
		- 公式 (4-17): 整个训练集的成本函数 $$J(θ) = - \frac{1}{m} \sum_{i=1}^{m} [ y^{(i)} \log(h(θ, x^{(i)})) + (1-y^{(i)}) \log(1-h(θ, x^{(i)}))]$$
	- 梯度下降： 和线性回归类似， 可以使用<mark style="background: #BBFABBA6;">梯度下降法</mark>来更新参数 θ ， 根据梯度方向调整 θ， <mark style="background: #BBFABBA6;">使得成本函数的值最小化</mark>。
		- 公式 (4-18) 成本函数关于 θj 的偏导数：
$$∇θj = \frac{1}{m} \sum_{i=1}^{m} (h(θ, x^{(i)}) - y^{(i)}) * x_j^{(i)}$$

- 如何训练逻辑回归模型
	- 使用梯度下降法 (Gradient Descent): 通过迭代地更新模型参数 θ 来最小化成本函数。
 
- 扩展到<mark style="background: #FFB8EBA6;">多类别分类</mark> (Softmax 回归)
	- Softmax 回归 (Multinomial Logistic Regression) 是逻辑回归模型的一种扩展， 可以用于解决多类别分类问题。
	- 原理: <mark style="background: #BBFABBA6;">Softmax 回归 计算每个类别的一个分数， 然后使用 Softmax 函数 将这些分数转换成概率。</mark>
		- 公式 (4-19) 类别 k 的分数: $s_k(x) = θ_k^T x$
		- $s_k(x)$ 代表 x 属于类别 k 的分数。
		- 公式 (4-20) Softmax 函数: $$\sigma(s(x))_k = \frac{e^{s_k(x)}}{\sum_{j=1}^K e^{s_j(x)}}$$
			- K 代表类别的数量， $σ(s(x))_k$ 代表样本 x 属于类别 k 的概率。
	- 预测结果: Softmax 回归 模型预测概率最大的类别。
	- 公式 (4-21): $$\hat{y} = \text{argmax}_k \sigma(s(x))_k$$
	- 成本函数: Softmax 回归模型的成本函数通常使用<mark style="background: #ADCCFFA6;"> 交叉熵函数 </mark>(Cross-Entropy)。
		- 公式 (4-22):
$$J(θ) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} y_k^{(i)} \log(h_k(θ, x^{(i)}))$$
	- 其中 $h_k(θ, x^{(i)})$ 代表模型预测 $x^{(i)}$ 属于类别 k的概率。
	- $y_k^{(i)}$ 是 $x^{(i)}$ 属于类别 k 的真实标签。
#### 4.7 练习题
*1.如果训练集具有数百万个特征，那么可以使用哪种线性回归训练算法？*

使用随机梯度下降或小批量梯度下降。如果训练集适合容纳于内存，则可以使用批量梯度下降。但是你不能使用标准方程法或SVD方法，因为随着特征数量的增加，计算复杂度会快速增长（超过二次方）。

*2.如果训练集里特征的数值大小迥异，哪种算法可能会受到影响？受影响程度如何？你应该怎么做？*

如果你的训练集中的特征具有不同的尺寸比例，则成本函数具有细长碗的形状，因此梯度下降算法需要很长时间才能收敛。为了解决这个问题，你应该在训练模型之前缩放数据。请注意，标准方程法或SVD方法无须缩放即可正常工作。此外，如果特征未按比例缩放，则正则化模型可能会收敛至次优解：由于正则化会惩罚较大的权重，因此与具有较大值的特征相比，具有较小值的特征往往会被忽略。

*3.训练逻辑回归模型时，梯度下降会卡在局部最小值中吗？

训练逻辑回归模型时，梯度下降不会陷入局部最小值，因为成本函数是凸函数

*4.如果你让它们运行足够长的时间，是否所有的梯度下降算法都能得出相同的模型？

如果优化问题是凸的（例如线性回归或逻辑回归），并且假设学习率不是太高，那么所有梯度下降算法都将接近全局最优并最终产生很相似的模型。但是，除非逐步降低学习率，否则随机梯度下降和小批量梯度下降将永远不会真正收敛。相反，它们会一直围绕全局最优值来回跳跃。这意味着即使你让它们运行很长时间，这些梯度下降算法也会产生略微不同的模型

*5.假设你使用批量梯度下降，并在每个轮次绘制验证误差。如果你发现验证错误持续上升，可能是什么情况？你该如何解决？

如果验证错误在每个轮次后持续上升，则一种可能性是学习率过高并且算法在发散。如果训练错误也增加了，那么这显然是问题所在，你应该降低学习率。但是，如果训练错误没有增加，则你的模型已经过拟合训练集，则应该停止训练。

*6.当验证错误上升时立即停止小批量梯度下降是个好主意吗？

由于随机性，随机梯度下降和小批量梯度下降都不能保证在每次训练迭代中都取得进展。因此，如果在验证错误上升时立即停止训练，则可能在达到最优值之前就停止太早了。更好的选择是按照一定的间隔时间保存模型。然后，当它很长时间没有改善（意味着它可能永远不会超过最优值）时，你可以恢复到保存的最佳模型。

*7.哪种梯度下降算法（在我们讨论过的算法中）将最快到达最佳解附近？哪个实际上会收敛？如何使其他的也收敛？

随机梯度下降法具有最快的训练迭代速度，因为它一次只考虑一个训练实例，因此它通常是第一个到达全局最优值附近的（或是很小批量大小的小批量梯度下降）。但是，给定足够的训练时间，实际上只有批量梯度下降会收敛。如前所述，随机梯度下降和小批量梯度下降会在最优值附近反弹，除非你逐渐降低学习率。

*8.假设你正在使用多项式回归。绘制学习曲线后，你会发现训练误差和验证误差之间存在很大的差距。发生了什么？解决此问题的三种方法是什么？

如果验证误差远高于训练误差，则可能是因为模型过拟合了训练集。解决此问题的一种方法是降低多项式阶数：较小自由度的模型不太可能过拟合。另一种方法是对模型进行正则化，例如，将 2（Ridge）或 1（Lasso）惩罚添加到成本函数。这也会降低模型的自由度。最后，你可以尝试增加训练集的大小。

*9.假设你正在使用岭回归，并且你注意到训练误差和验证误差几乎相等且相当高。你是否会说模型存在高偏差或高方差？你应该增加正则化超参数α还是减小它呢？

如果训练误差和验证误差几乎相等且相当高，则该模型很可能欠拟合训练集，这意味着它具有很高的偏差。你应该尝试减少正则化超参数α。

*10.为什么要使用：
	a.岭回归而不是简单的线性回归（即没有任何正则化）？
	b.Lasso而不是岭回归？
	c.弹性网络而不是Lasso？

具有某些正则化的模型通常比没有任何正则化的模型要好，因此，你通常应优先选择岭回归而不是简单的线性回归。

Lasso回归使用 1惩罚，这通常会将权重降低为零。这将导致稀疏模型，其中除了最重要的权重之外，所有权重均为零。这是一种自动进行特征选择的方法，如果你怀疑实际上只有很少的特征很重要，那么这是一种很好的方法。如果你不确定，则应首选岭回归。

与Lasso相比，弹性网络通常更受青睐，因为Lasso在某些情况下可能产生异常（当几个特征强相关或当特征比训练实例更多时）。但是，它确实增加了额外需要进行调整的超参数。如果你希望Lasso没有不稳定的行为，则可以仅使用l1_ratio接近1的弹性网络。

*11.假设你要将图片分类为室外/室内和白天/夜间。你应该实现两个逻辑回归分类器还是一个Softmax回归分类器？

如果你要将图片分类为室外/室内和白天/夜间，因为它们不是排他的类（即所有四种组合都是可能的），则应训练两个逻辑回归分类器。

*12.用Softmax回归进行批量梯度下降训练，实现提前停止法。
```python
import numpy as np
from sklearn import datasets
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 加载 Iris 数据集
iris = datasets.load_iris()
X = iris["data"][:, (2, 3)]  # Petal length, petal width
y = iris["target"]

# 添加偏置项
X_with_bias = np.c_[np.ones([len(X), 1]), X]

# 将数据集划分成训练集、验证集和测试集
X_train, X_valid, y_train, y_valid = train_test_split(
    X_with_bias, y, test_size=0.2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(
    X_train, y_train, test_size=0.25, random_state=42)

# 创建 Softmax 回归模型
softmax_reg = LogisticRegression(
    multi_class='multinomial', solver='lbfgs', C=10, random_state=42)

# 使用提前停止法进行训练
# 设置提前停止参数
early_stopping = True  # 如果设置为 True，则使用提前停止法
n_iterations_without_improvement = 5  # 如果 validation loss 在 n_iterations_without_improvement 次迭代中没有改善，则停止训练
patience = 500  # 在验证 loss 停止改善之后，继续训练的迭代次数
best_loss = np.infty  # 记录验证集上的最佳损失值
iterations_without_improvement = 0  # 记录连续多少次没有看到验证集 loss 的改善

for epoch in range(5001):
    # 在训练集上训练模型
    softmax_reg.fit(X_train, y_train)

    # 在验证集上评估模型
    y_pred = softmax_reg.predict(X_valid)
    loss = np.mean(y_pred != y_valid)
    if loss <= best_loss:
        # 如果验证集上的损失值改善，则重置  `iterations_without_improvement`   为  0，   并记录  `best_loss`
        iterations_without_improvement = 0
        best_loss = loss
    else:
        # 如果验证集上的损失值没有改善，则增加  `iterations_without_improvement`   的值
        iterations_without_improvement += 1

    if epoch % 500 == 0:
        print(epoch, loss)

    #  提前停止
    if early_stopping and iterations_without_improvement >= n_iterations_without_improvement:
        print("Early stopping!")
        break

    
# 进行最终测试
y_pred = softmax_reg.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Test accuracy:", accuracy)
```
### 第五章 支持向量机
#### 5.1 线性SVM分类
- 核心思想：<mark style="background: #FFB8EBA6;">大间隔分类</mark>
	- SVM 的目标: 找到一个可以将数据点完美分类的决策边界， 并且距离最近的点最远 **。 换句话样说， SVM 希望找到一个最大间隔的分类器。
	- 直观解释: 想象你有一堆红点和蓝点，你需要用一条线将它们分开。 线性 SVM 会找到一条线， <mark style="background: #ABF7F7A6;">使得这条线到最近红点和最近蓝点的距离最大</mark>， 从而使得分类更稳健。![[Pasted image 20250106022627.png]]
- 数学原理
	- **间隔 (Margin)**: 间隔是指<mark style="background: #ABF7F7A6;">决策边界到最近的数据点之间的距离</mark>。
	- **支持向量 (Supporting Vector)**: 距离<mark style="background: #ABF7F7A6;">决策边界最近的那些数据点</mark>， 被称为支持向量。
	- **最大间隔**: ==SVM 的目标就是找到能够最大化间隔的决策边界==。
	- **硬间隔分类**: 当数据*完全线性可分*时， 我们可以找到一个 硬间隔 (Hard Margin)， 即所有数据点都在自己正确分类的半空间中， 且距离决策边界最远。
	- **软间隔分类**: 当数据*不能完全线性可分*时， 我们通常会引入 软间隔 (Soft Margin)。 <mark style="background: #ABF7F7A6;">软间隔允许一些数据点越过决策边界， 但会对其进行惩罚</mark>。
	- **成本函数**: 线性 SVM 的成本函数通常使用 *Hinge 损失函数*。 Hinge 损失函数的目的是惩罚那些距离决策边界很近的数据点。
	- 模型参数: 线性 SVM 的模型参数可以写成 θ (包括 w 和 b)， w 决定了决策边界的方向， b 决定了决策边界的截距。
		- 决策边界的方程: $w^Tx + b = 0$
		- 数据点分类规则:
			- $w^Tx + b > 0$: 预测为正类 (1)
			- $w^Tx + b < 0$: 预测为负类 (0)
- 训练线性 SVM
	- 可以使用 梯度下降法 来更新模型参数 θ， 使得成本函数的值最小化。
#### 5.2 非线性SVM分类
- 问题：非线性可分数据
	- 许多现实世界的数据集并不是线性可分的， 例如:
		- 卫星数据集：图像中的数据点分布类似于两个交织的半圆形
		- 其他真实世界数据：手写数字识别、 基因分析等等。

- 解决方法：<mark style="background: #FFB8EBA6;">核技巧 (Kernel Trick)</mark>
	- 添加特征： 可以将原始数据特征转换成更复杂的特征（例如多项式特征）， 使数据变得线性可分。 但这种方法可能会导致特征数量爆炸。
	- 核技巧: 核技巧可以帮助我们隐式地转换数据特征， 而不需要显式地创建这些特征。 这让我们能够使用线性 SVM 来处理非线性数据。 核技巧相当于将数据映射到更高的维度空间中， 在这个空间中，数据可能更容易线性可分。
	- <mark style="background: #FFB8EBA6;">内核函数</mark>: 内核函数 $K(x_1, x_2)$ 定义了两个数据点 $x_1$ 和 $x_2$ 之间在高维空间中的相似度， 它隐式地计算了数据在高维空间中的内积。

- 常用的内核函数
	- ==多项式核 (Polynomial Kernel)==
$$K(x_1, x_2) = (γx_1^T x_2 + coef0)^{degree}$$
		- γ, coef0, degree 是超参数。
		- 高阶多项式核可以创建更多 "<mark style="background: #ABF7F7A6;">弯曲" 的决策边界</mark>。

	- ==高斯径向基函数核 (RBF Kernel)==
$$K(x_1, x_2) = exp(-\frac{||x_1 - x_2||^2}{2γ^2 })$$
		- γ 是超参数。
		- 高斯 RBF 核 可以创建<mark style="background: #ABF7F7A6;">更加灵活的非线性决策边界， 它可以很好地处理样本之间的局部变化。</mark>
$$[ \phi(x, \text{landmark}) = \exp\bigl(-\gamma |x - \text{landmark}|^2\bigr). ]$$
- 模型选择和超参数
	- 选择合适的核函数 (linear, polynomial, rbf, etc.)
	- 调整超参数: C (正则化強度), degree (多项式核), γ (RBF 核), coef0 (多项式核的常数项)
	- 使用交叉验证 (Cross-validation) 来寻找最佳参数。

- 计算复杂度
	- LinearSVC: 使用 liblinear 库， 时间复杂度大约为 O($m \times n$)。
	- SVC: 使用 libsvm 库， 时间复杂度大约为 O($m^2 \times n$) 到 O($m^3 \times n$)。
	- 如果数据集很大， 你可能需要考虑 LinearSVC。 如果数据集较小， SVC 可以提供更高精度。
#### 5.3 SVM回归
- 核心思想：<mark style="background: #FFB8EBA6;">间隔回归 (ε-Insensitive)</mark>
	- 目标: 找到一条线 (决策边界)， 使尽可能多的数据点落在 **间隔** 范围内， 同时限制那些落在间隔外的 **间隔违例** 个数。
	- <mark style="background: #ABF7F7A6;">间隔 (Margin): 由超参数 ε 控制的宽度。</mark>
	- ε 不敏感: 在间隔内的点不会影响模型的预测， 因为<mark style="background: #ADCCFFA6;">模型的预测取决于间隔边界， 即支持向量。</mark>
-  数学原理
	- 成本函数: SVM 回归的成本函数通常使用 <mark style="background: #ABF7F7A6;">ε- 不敏感损失函数</mark> (Epsilon-Insensitive loss function)。 此损失函数只对落在间隔外的点进行惩罚， 对于间隔内的点， 损失值为 0。
	- 公式: $L(y, f(x)) = max(0, |y - f(x)| - ε)$ 其中:
		- y 表示真实值。
		- f(x) 表示模型对 x 的预测值。
		- ε 表示间隔宽度。
	- 模型参数: SVM 回归的模型参数是 θ (包括 w 和 b)。 w 决定了决策边界的方向， b 决定了决策边界的截距。
- 超参数
	- C (正则化强度): 类似于之前的 SVM 分类器， C 控制着模型的复杂度。
	- ε (间隔宽度): 控制着模型对异常值的敏感度。
	- gamma (RBF 核), degree (多项式核): 这些参数和之前的非线性 SVM 分类器类似。
#### 5.4 工作原理
##### 5.4.1 决策函数和预测
- 线性 SVM 分类器使用一个简单的决策函数 f(x) 来预测新实例 x 的类别:
	- 公式 (5-2): $f(x) = w^T x + b$
		- w 是权重向量。
		- b 是截距 (偏置项)
		- x 是样本特征向量。
	- 预测规则:
		- f(x) > 0 : 预测为正类 (1)
		- f(x) < 0: 预测为负类 (0)
##### 5.4.2 训练的目标：最大化间隔
- 目标函数: SVM 的训练的目标是找到一个可以最大化间隔的决策边界， 同时限制间隔冲突。
- 硬间隔: 当所有样本点可以被一条决策线完美分离时， 找到硬间隔 (Hard Margin):
	- $w^Tx + b ≥ 1$ (对于正类样本)
	- $w^Tx + b ≤ -1$ (对于负类样本)
- 软间隔: 当数据不能完美分离时， 允许一些样本点越过决策边界， 但加惩罚。 这部分由 C 参数来控制。
	- 公式 (5-4): 目标函数 (Soft Margin): $min \frac{2}{1} w^T w  + C * \sum_{i=1}^m ζ(i)$
	- 约束条件: 
		- $w^Tx(i) + b ≥ 1 - ζ(i)$ (对于正类样本)
		- $w^Tx(i) + b ≤ -1 + ζ(i)$ (对于负类样本)
		- $ζ(i) ≥ 0$ 是<mark style="background: #FFB8EBA6;">松弛变量 (Slack Variable)， 代表数据点违反间隔的要求程度。</mark>
##### 5.4.3 二次规划 Quadratic Programming
- **解决方法:** SVM 的训练问题可以转化为一个 **二次规划 (Quadratic Programming)** 问题， 可以使用 **Quadratic Program (QP) 求解器** 来求解。
- QP 问题的形式如下:
    - **公式 (5-5):** $min \frac{1}{2} p^T H p + f^T p$
        - 约束条件: $Ap <=b$
- 可以将硬间隔和软间隔 SVM 的训练问题都表示成 QP 问题， 并将相应参数代入 QP 问题中。
##### 5.4.4 对偶问题 Dual Problem
- **原始问题和对偶问题**: 对于任何约束优化问题，都有与之对应的对偶问题 (Dual Problem)。
	- 对偶问题的解 通常是 原问题的解的下界 (Lower Bound)。
	- 在某些情况下，对偶问题的最优解与原问题的最优解相同。
- SVM 问题: SVM 的训练问题满足对偶问题的条件， 所以可以利用 对偶问题 来求解。
	- 在某些情况下， 解决对偶问题比解决原问题更快。
	- 对偶问题 还可以更容易地应用 核技巧。
- 线性 SVM 的对偶问题:
$$min \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m α^{(i)} α^{(j)} t^{(i)} t^{(j)} x^{(i)T} x^{(j)}-  \sum_{i=1}^m \alpha^{(i)}$$
	- 约束条件:  $α^{(i)} ≥ 0$ 
		- α(i) 是拉格朗日乘子 (Lagrange Multiplier)。
		- t(i) 是第 i 个样本的标签 (1 或 -1)。
		- x(i) 是第 i 个样本的特征向量。
##### 5.4.5 核技巧
- **核心思想:** 通过使用 **核函数** 来<mark style="background: #FFB8EBA6;">隐式地在高维空间中计算数据点的内积</mark> (点积)， 从而避免显式地将数据映射到高维空间。 这可以提高计算效率， 特别是在处理高维特征时。
- **步骤:**
1. 选择一个合适的核函数 (比如 $K(x1, x2)$)。 常见的核函数包括 "polynomial", "rbf", "linear", "sigmoid"。
2. 将对偶问题中的 $x(i)^T x(j)$ 替换成 $K(x(i) , x(j))$。 这样就能使用核函数来进行计算， 不需要显式地映射到高维空间。
![[Pasted image 20250106192055.png]]
- **预测:**
![[Pasted image 20250106192205.png]]
##### 5.4.6 在线SVM
- 在线 SVM: 使用 **增量 (Incremental)** 的方式来训练 SVM 模型， 不断地接收新的样本， 并更新模型参数。
- 方法:
	- 使用 SGDClassifier (Stochastic Gradient Descent Classifier) 来训练 线性 SVM 模型。
	- 使用 **梯度下降法 (Gradient Descent)** 来最小化 SVM 成本函数:
$$J(w, b) = (1/2) * w^Tw + C * \sum_{i=1}^m max(0 , 1 - t(i)(w^T x(i) + b))$$

		- C 是正则化系数， 控制着模型的复杂度。
		- w 是权重向量。
		- b 是偏置项。
#### 5.5 练习题
*1.支持向量机的基本思想是什么？ 

支持向量机的基本思想是使决策边界之间的间隔最大化，该决策边界分隔两个类别和训练实例。SVM执行软间隔分类时，实际上是在完美分隔两个类和拥有尽可能最宽的街道之间寻找折中方法（也就是允许少数实例最终还是落在街道上）。还有一个关键点是在训练非线性数据集时，记得使用核函数。

*2.什么是支持向量？ 

支持向量机的训练完成后，位于“街道”（参考上一个答案）之上的实例被称为支持向量，这也包括处于边界上的实例。决策边界完全由支持向量决定。非支持向量的实例（也就是街道之外的实例）完全没有任何影响。你可以选择删除它们然后添加更多的实例，或者将它们移开，只要一直在街道之外，它们就不会对决策边界产生任何影响。计算预测结果只会涉及支持向量，而不涉及整个训练集。

*3.使用SVM时，对输入值进行缩放为什么重要？ 

支持向量机拟合类别之间可能的、最宽的“街道”（参考第1题答案），所以如果训练集不经缩放，SVM将趋于忽略值较小的特征

*4.SVM分类器在对实例进行分类时，会输出信心分数吗？概率呢？ 

支持向量机分类器能够输出测试实例与决策边界之间的距离，你可以将其用作信心分数。但是这个分数不能直接转化成类别概率的估算。如果创建SVM时，在Scikit-Learn中设置probability=True，那么训练完成后，算法将使用逻辑回归对SVM分数进行校准（对训练数据额外进行5-折交叉验证的训练），从而得到概率值。这会给SVM添加predict_proba（）和predict_log_proba（）两种方法。

*5.如果训练集有成百万个实例和几百个特征，你应该使用SVM原始 问题还是对偶问题来训练模型？ 

这个问题仅适用于线性支持向量机，因为核SVM只能使用对偶问题。对于SVM问题来
说，原始形式的计算复杂度与训练实例m的数量成正比，而其对偶形式的计算复杂度与某个介于m2和m3之间的数量成正比。所以如果实例的数量以百万计，一定要使用原始问题，因为对偶问题会非常慢。

*6.假设你用RBF核训练了一个SVM分类器，看起来似乎对训练集欠 拟合，你应该提升还是降低γ（gamma）？C呢？ 

如果一个使用RBF核训练的支持向量机对欠拟合训练集，可能是由于过度正则化导致的。你需要提升gamma或C（或同时提升二者）来降低正则化。

*7.如果使用现成二次规划求解器，你应该如何设置QP参数（H、 f、A和b）来解决软间隔线性SVM分类器问题？ 
![[Pasted image 20250106201554.png]]![[Pasted image 20250106201736.png]]
*8.在一个线性可分离数据集上训练LinearSVC。然后在同一数据集 上训练SVC和SGDClassifier。看看你是否可以用它们产生大致相同的 模型。 

*9.在MNIST数据集上训练SVM分类器。由于SVM分类器是个二元分类 器，所以你需要使用一对多来为10个数字进行分类。你可能还需要使 用小型验证集来调整超参数以加快进度。最后看看达到的准确率是多 少？ 

*10.在加州住房数据集上训练一个SVM回归模型。

### 第六章 决策树
- 决策树概述：
	- 决策树是一种通用的机器学习算法，可以用于分类、回归和多输出任务。
	- 决策树可以拟合复杂的数据集。
	- 决策树是随机森林的重要组成部分。
- 训练和可视化决策树：
	- 通过一系列 "问答" 来判断事物所属类别， 就像一个流程图。
	- 使用已有的数据来 "教" 决策树如何选择 "问题" 和 "答案"。
	- 使用 `DecisionTreeClassifier` 来训练决策树模型。
	- `export_graphviz`方法可以把决策树转换为图形， 以便于观察和分析。
	- `Graphviz` 软件包可以用来将图形定义文件 (.dot) 转换为其他格式 (PDF or PNG).
#### 6.1 训练和可视化决策树
- 代码示例用 鸢尾花 数据集来训练 `DecisionTreeClassifier` 模型。
- 模型的 `max_depth` 参数控制决策树的深度。
- 使用 `export_graphviz()` 函数将决策树导出为图形定义文件 `.dot`。
- 使用 `Graphviz` 工具将 `.dot` 文件转换为 PNG 图像文件。
#### 6.2 做出预测
- **核心步骤**
	- *从根节点开始*： 预测过程是从决策树的根节点开始的。
	- *根据问题选择分支*： 每个节点都包含一个问题，根據当前数据，选择与当前数据匹配的分支。
	- *继续向下移动*： 在新的节点上， 继续按照步骤 2 进行判断，直到到达一个叶子节点。
	- *叶子节点即预测结果*： 叶子节点只包含一个预测类别， 该类别就是决策树最终的预测结果。
- **例 1: 使用决策树判定 "山鸢尾"**
	- 找到一朵花瓣长度小于 2.45cm 的鸢尾花。
	- 从根节点开始， 第一个问题是 "花瓣长度小于 2.45cm?"， 答案是 "是"。
	- 根据答案，向下移动到根节点的左子节点。
	- 该子节点是一个叶子节点， 直接显示预测类型为 `山鸢尾。
- **例 2: 使用决策树判定 "变色鸢尾" 或 "维吉尼亚鸢尾"**
	- 找到一朵花瓣长度大于 2.45cm 的鸢尾花。
	- 从根结点开始，第一个问题是 "花瓣长度小于 2.45cm?"， 答案是 "否"。
	- 根据答案， 向下移动到根节点的右子节点。
	- 该子节点不是叶子节点， 会提出另一个问题: "花瓣宽度小于 1.75cm?"。
		- 如果答案是 "是", 预测类型为 `变色鸢尾`。
		- 如果答案是 "否", 预测类型为 `维吉尼亚鸢尾`。
- **决策树的特点**
	- 数据准备: 决策树一般不需要对数据进行 特征缩放 或 中心化处理 (归一化)， 因为它是一种基于数据分割的算法。
	- 样本数量和值: samples 属性表示当前节点使用的训练样本数量。 value 属性表示当前节点上每个类别的样本数量。
	- <mark style="background: #ABF7F7A6;">基尼系数 (Gini):</mark> gini 属性用于衡量节点上的<mark style="background: #FFB8EBA6;">不纯度</mark>。![[Pasted image 20250115203430.png]]
		- 如果所有样本都属于同一类别， 那么节点就称为 "纯" 节点， gini 值为 0。
		- <mark style="background: #ADCCFFA6;">如果样本分属于多个类别， 那么节点就是 "不纯"， gini 值就更大</mark>。
- **CART 算法**
	- Scikit-Learn 中使用 "CART (<mark style="background: #ABF7F7A6;">Classification and Regression Trees</mark>)" 算法来生成二叉树， 它通常用于构建决策树模型。
	- <mark style="background: #FFB8EBA6;">非叶子节点 </mark>只有两个子节点。
	- <mark style="background: #FFB8EBA6;">ID3 算法</mark> 可以生成具有多个子节点的决策树。
- 可解释性：
	- 白盒模型: 决策树属于 白盒模型， 因为它决策方式直观易懂， 可以清楚地了解模型是如何做出预测的。
	- 黑盒模型: 相比之下， 随机森林 或 神经网络 属于 黑盒模型， 它们预测效果很好， 但是难以解释模型预测的内部逻辑。
- 图 6-2：决策边界![[Pasted image 20250115203340.png]]
	- 图 6-2 显示了决策树的决策边界如何随着深度增加而变复杂。
	- 由于 最大深度 max_depth 被设置为 2， 决策树在达到 max_depth 时停止分割。
	- 如果 `max_depth` 设置为 3， 决策树会进行更多分割， 创建更复杂的决策边界。
#### 6.3 估计类概率
- 核心概念
	- 概率估计: 决策树除了进行分类预测（即预测样本属于哪个类别），还可以估计每个类别出现的概率。
	- 叶子节点的概率: 决策树通过计算叶子节点中每个类别样本的比例来进行概率估计。
	- 预测类别的选择: 决策树通常选择概率最高的类别作为最终的预测结果。
- 代码示例
	- 这段文字使用一个例子来解释概率估计的过程：
	- 假设找到一朵花， 花瓣长度为 5cm， 宽度为 1.5cm。
	- 根据决策图， 它最终会被归类到 深度 2 左侧 节点。
	- 该节点包含 54 个样本，其中 0 个是 山鸢尾、 49 个是 变色鸢尾、 5 个是 维吉尼亚鸢尾。
	- 因此， 决策树会输出以下概率：
		- 山鸢尾： 0% (0/54)
		- 变色鸢尾: 90.7% (49/54)
		- 维吉尼亚鸢尾: 9.3% (5/54)
- 代码使用
	- `tree_clf.predict_proba([[5, 1.5]])` : 该代码利用决策树模型 tree_clf 进行概率估计，输入数据是一个包含花瓣长度和宽度的样本。
	- `predict_proba` 方法返回一个概率矩阵， 矩阵的行数等于样本数， 列数等于类别的数量， 每个元素代表该样本属于某一类别的概率。
	- `tree_clf.predict([[5, 1.5]])` : 该代码利用决策树模型 tree_clf 进行类别预测，也输入一个样本。
	- `predict` 方法直接返回预测的类别， 该类别是 变色鸢尾 (类别 1)， 因为它的概率最高 (90.7%)。
#### 6.4 CART训练算法
- **CART (Classification and Regression Trees)**: 分类与回归树算法， 它是一种贪婪算法， 用于构建决策树模型。![[Pasted image 20250115210219.png]]
- 算法步骤:
	- 首先选择一个特征 k 和一个阈值 tk ， 将训练数据集分为两个子集。
	- 选择 k 和 tk 的标准是： 能够产生纯度最高的子集 (子集中的样本尽可能地属于同一类别)。
	- 然后递归地对子集进行分割， 直到满足以下条件之一：
		- 达到最大深度 (max_depth) 。
		- 找不到能够减少不纯度的分割。
- **成本函数**: CART 算法使用一个成本函数来评估分割的效果。 <mark style="background: #FFB8EBA6;">它的目标是找到能够最小化成本函数的 k 和 tk。</mark>
- **贪婪算法**: CART 是一种贪婪算法， 它在每层寻找最优分割， 而不会回头检查之前的分割是否是最优的。 因此， 它能找到一个 "相当不错的" 解， 但不能保证是最优解。
- **NP 完全问题**: 找到 "最优" 树是一个 NP 完全问题， 这意味着找到最优解的计算复杂度非常高， 对于大型数据集来说非常困难。
- 其他停止条件: max_depth 之外， Scikit-Learn 提供了其他超参数来控制决策树的停止条件， 例如:
	- min_samples_split: 决定最小的样本数， 这些样本必须存在才能执行新的分割。
	- min_samples_leaf: 每个叶节点最小的样本数。
	- min_weight_fraction_leaf: 每个叶节点样本总数的最小的权重分数。
	- max_leaf_nodes: 允许的最大叶节点数。
#### 6.5 计算复杂度
- 预测复杂度:
	- 预测过程需要从根节点到叶子节点进行遍历。
	- 决策树一般是近似平衡的， 因此遍历时间复杂度为 `O (log2 (m))`，其中 m 是样本数量。
	- 由于每个节点只需检查一个特征， 因此总的预测复杂度为 `O (log2 (m))`，与特征数量无关。
	- 这意味着即使对大型数据集， 决策树的预测速度仍然很快。
- 训练复杂度:
	- 训练过程需要比较每个节点上所有样本的所有特征 (如果设置了 max_features， 则比较更少的特征)。
	- 训练复杂度为 `O (n * m * log2 (m))`， 其中 n 是特征数量， m 是样本数量。
	- 对于小型数据集， 可以通过设置 presort=True 来加速训练， 但这会降低大型数据集的训练速度。
#### 6.6 基尼不纯度或熵
- 两种不纯度测量方法:
	- **基尼不纯度**: Scikit-Learn 默认使用基尼不纯度来测量节点的不纯度。
	- **熵**: 可以使用 criterion="entropy" 来选择熵作为不纯度的指标。![[Pasted image 20250115213247.png]]
- 概念:
	- 熵: 在信息论中， 熵用来衡量一个随机变量的不确定性。 信息量越大， 熵值越大；信息量越小， 熵值越小。 一个完全确定的事件的熵值为 0。
	- 基尼不纯度: 基尼不纯度用来衡量数据集中样本类别的不纯度。
- 公式: 文中给出了基尼不纯度和熵的计算公式。
- 区别:
	- 基尼不纯度倾向于生成规模较小的树， 而熵则更偏向于生成更平衡的树。
	- 基尼不纯度的计算速度略快。
	- 在大多数情况下， 使用基尼不纯度或熵的影响不大， 它们产生的决策树并没有明显的区别。
#### 6.7 正则化超参数
- 要点概述:
	- 决策树的特点: 决策树模型很少对数据做出假设， 因此它可以拟合各种复杂的数据集。 但这也会导致它容易过拟合， 因为它会过度地学习训练数据中的细节， 而忽略了数据集的整体规律。
	- 过拟合: 模型在训练集上表现很好， 但在验证集上表现很差的现象称为过拟合。
	- 正则化: 正则化是用来降低模型过拟合风险的方法。
	- 决策树的正则化: 可以通过限制决策树的复杂度来防止过拟合。
	- 正则化超参数: Scikit-Learn 使用 max_depth, min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_leaf_nodes, max_features 这些参数来控制决策树的复杂度。
- 具体超参数解释:
	- max_depth: 决策树的最大深度。 值越小， 树越简单， 过拟合风险越低。
	- min_samples_split: 为了执行分割， 节点必须至少包含的样本数量。 值越大， 分割越少， 树越简单， 过拟合风险越低。
	- min_samples_leaf: 每个叶节点至少包含的样本数量。 值越大， 叶子节点越少， 整体树结构更简单， 过拟合风险越低。
	- min_weight_fraction_leaf: 与 min_samples_leaf 相似， 但不是样本数量， 而是样本的权重之和。 这是一个更有用的指标， 尤其是在类别分布不平衡的情况下。
	- max_leaf_nodes: 允许的最大叶节点数。 值越小， 叶节点越少， 树结构更简单， 过拟合风险越低。
	- max_features: 每个节点分割时所考虑的最大特征数。 值越小， 每个节点考虑的特征越少， 树越简单， 过拟合风险越低。
- 剪枝 (pruning)
	- 剪枝是对已经训练好的决策树进行简化， 删除不必要的节点。
	- 剪枝可以通过统计测试 (例如 χ2 检验) 来评估节点是否重要， 如果节点的子节点都是叶节点， 并且该节点没有显著提高树的纯度， 那么这个节点就可以被删除。
#### 6.8 回归
- 要点概述:
	- 决策树回归: 决策树不仅可以用于分类， 还可以用于回归任务。
	- 回归目标: 回归任务的目标是预测连续的数值， 比如房价、气温等等。
	- CART 算法: Scikit-Learn 使用 CART 算法来训练决策树模型， <mark style="background: #FFB8EBA6;">在回归任务中， CART 使用均方误差 (MSE) 作为成本函数</mark>， 用来评估分割的效果。
	- 成本函数: 成本函数的公式如下：![[Pasted image 20250115222529.png]]
$$J(k, t_k) = (m_{left}/m) * MSE_{left} + (m_{right}/m) * MSE_{right}$$
		- J(k, tk) 是分割成本。
		- k 是用来进行分割的特征。
		- tk 是分割的阈值。
		- m 是样本总数。
		- m_left 和 m_right 分别表示分割后左右子集的样本数量。
		- MSE_left 和 MSE_right 分别表示分割后左右子集的均方误差。
	- 均方误差 : 均方误差用来衡量预测值与真实值之间的差异。
	- 正则化: 在回归任务中， 决策树也容易过拟合， 因为节点的分割会非常细致， 导致模型过于复杂， 无法很好地泛化到新数据。 可以使用正则化来解决这个问题， 方法是限制树的复杂度， 例如使用 min_samples_leaf 参数来指定叶节点的最小样本数量， 从而保证每个叶子节点上都有足够的样本， 防止出现过拟合并提高模型泛化能力。
	- 图 6-6: 该图展示了如何使用 min_samples_leaf 参数来正则化一个回归决策树，左边的模型没有正则化， 右边模型设置 min_samples_leaf = 10 来限制叶节点最小样本数。 可以看出， 经过正则化的模型能够更好地泛化到新数据。![[Pasted image 20250115222454.png]]
#### 6.9 不稳定性
- 要点概述:
	- 决策树不稳定: 决策树模型对训练数据的微小变化非常敏感。
	- 不稳定的表现: 如果训练数据的顺序改变， 或者训练数据集中仅仅删除或添加几个样本， 就会导致最终训练出来的决策树模型结构发生很大的变化。
	- 图 6-7 和 6-8: 
		- 图 6-7 展示了同一数据集在不同的数据顺序下训练出来的两种不同的决策树。 
		- 图 6-8 展示了更细微的例子： 仅仅是将一个样本从 "花萼长度" 为 5cm 移动到 4cm 就会导致决策树模型产生很大的变化 (图中灰色的区域代表不同决策树的预测结果)。
	- 不稳定性的危害: 这种不稳定性会导致模型的泛化性能下降， 因为模型对训练数据过于敏感， 而忽略了数据中的真实规律。
	- 解决方法:
		- 随机森林: 随机森林算法可以通过多次创建不同的决策树， 然后对这些树的预测结果进行平均来降低不稳定性，从而提高模型的泛化能力。
#### 6.10 练习题
*1.如果训练集有100万个实例，训练决策树（无约束）大致的深度 是多少？ 

一个包含m个叶节点的均衡二叉树的深度等于log2（m）（注：log2是基2对数，log2（m）=log（m）/log（2）。），取整。通常来说，二元决策树（只做二元决策的树，就像Scikit-Learn中的所有树一样）训练到最后大体都是平衡的，如果不加以限制，最后平均每个叶节点一个实例。因此，如果训练集包含100万个实例，那么决策树的深度为log2（106）≈20层（实际上会更多一些，因为决策树通常不可能完美平衡）。

*2.通常来说，子节点的基尼不纯度是高于还是低于其父节点？是 通常更高/更低？还是永远更高/更低？ 
![[Pasted image 20250115223353.png]]![[Pasted image 20250115223401.png]]

*3.如果决策树过拟合训练集，减少max_depth是否为一个好主意？ 

如果决策树过拟合训练集，降低max_depth可能是一个好主意，因为这会限制模型，使其正则化。

*4.如果决策树对训练集欠拟合，尝试缩放输入特征是否为一个好 主意？ 

决策树的优点之一就是它们不关心训练数据是缩放还是集中，所以如果决策树不适合训练集，缩放输入特征不过是浪费时间罢了。

*5.如果在包含100万个实例的训练集上训练决策树需要一个小时， 那么在包含1000万个实例的训练集上训练决策树，大概需要多长时 间？ 
![[Pasted image 20250115223517.png]]

*6.如果训练集包含10万个实例，设置presort=True可以加快训练 吗？ 

只有当数据集小于数千个实例时，预处理训练集才可以加速训练。如果包含100 000个实例，设置presort=True会显著减慢训练。

*7.为卫星数据集训练并微调一个决策树。 
a.使用make_moons（n_samples=10000，noise=0.4）生成一个卫 星数据集。 
b.使用train_test_split（）拆分训练集和测试集。 
c.使用交叉验证的网格搜索（在GridSearchCV的帮助下）为 DecisionTreeClassifier找到适合的超参数。提示：尝试 max_leaf_nodes的多种值。
d.使用超参数对整个训练集进行训练，并测量模型在测试集上的 性能。你应该得到约85%～87%的准确率。 

*8.按照以下步骤种植森林。 
a.继续之前的练习，生产1000个训练集子集，每个子集包含随机 挑选的100个实例。提示：使用Scikit-Learn的ShuffleSplit来实现。 
b.使用前面得到的最佳超参数值，在每个子集上训练一个决策 树。在测试集上评估这1000个决策树。因为训练集更小，所以这些决 策树的表现可能比第一个决策树要差一些，只能达到约80%的准确率。 
c.见证奇迹的时刻到了。对于每个测试集实例，生成1000个决策 树的预测，然后仅保留次数最频繁的预测（可以使用SciPy的mode（） 函数）。这样你在测试集上可获得大多数投票的预测结果。 
d.评估测试集上的这些预测，你得到的准确率应该比第一个模型 更高（高出0.5%～1.5%）。恭喜，你已经训练出了一个随机森林分类 器！ 

### 第七章 集成学习和随机森林
#### 7.1 投票分类器 Voting Classifiers
- 投票分类器是集成学习中最简单的一种策略。
- 它将多个分类器的预测结果进行投票，以获得最终的预测类别。
- 投票方式分为两种:
	- <mark style="background: #FFB8EBA6;">硬投票 (Hard Voting):</mark> 直接对每个分类器的预测结果进行投票，选择得票最多的类别作为最终预测结果。
	- <mark style="background: #FFB8EBA6;">软投票 (Soft Voting)</mark>: 对每个分类器预测的概率进行平均，然后选择平均概率最高的类别作为最终预测结果.
##### 7.1.1 集成学习的优势 (Benefits of Ensemble Learning):
- 投票分类器的准确率通常比它包含的单个分类器中的最佳分类器还要高.
- 即使每个分类器都是 “弱学习器” （意味着它们的准确率只比随机猜测高一点点）， 通过集成学习还是可以实现一个 “强学习器” （高准确率）, 只要有足够多的且种类足够丰富的弱学习器。
##### 7.1.2 类比： 抛硬币 (Coin Toss Analogy):
- 就像抛一 m个略微偏向正面的硬币， 即使正面朝上的概率只有 51%， 但如果抛很多次， 正面朝上的次数最终会超过 50%。
- 这说明即使每个预测器都有一定程度的不准确性， 但它们之间的独立性可以帮助集成学习算法提升整体的准确率。
##### 7.1.3 集成学习中的独立性 (Independence in Ensemble Learning):
- 集成学习的效果与每个预测器之间的*独立性*密切相关。
- 当预测器之间尽可能独立，它们犯错的类型尽可能不同时，集成方法的效果最好。
- 我们可以通过使用*不同的算法*来训练多个预测器， 以提高它们之间的独立性。
##### 7.1.4 代码示例:
- 代码示范了如何在 Scikit-Learn 中创建并训练一个由 *逻辑回归, 随机森林, 支持向量机* 三种分类器组成的投票分类器。
- 代码对比了三种单个分类器和投票分类器的准确率，展示了投票分类器的优势。
- 代码还修改了投票方式， 从 *硬投票* 改为 *软投票*， 用 predict_proba() 方法来进行概率估计。
#### 7.2 bagging 和 pasting
- 目标: 通过在<mark style="background: #BBFABBA6;">训练数据的不同随机子集上训练多个预测器来提高模型的泛化能力</mark>。
- 区别:
	- **Bagging**: 在采样时 *放回* 样本。 这意味着单个样本可能被同一个预测器多次选择， 导致模型之间存在一定程度的依赖。
	- **Pasting**: 在采样时 *不放回* 样本。 每个样本最多只被一个模型选择， 模型之间更独立。
- 训练过程: 图 7-4 展示了 Bagging 和 Pasting 的采样和训练过程。![[Pasted image 20250116213048.png]]
	- Bagging 和 Pasting 都允许训练样本被多个预测器使用， 但只有 Bagging 允许单个样本被同一个预测器多次使用。
##### 7.2.1 偏差 - 方差权衡 (Bias-Variance Trade-off):
- 单个模型 vs. 集成模型: 每个单个预测器的偏差可能高于直接在原始训练集上训练的模型， 但通过聚合多个预测器， 可以同时降低偏差和方差.
- 最终结果: <mark style="background: #ABF7F7A6;">集成模型的偏差与单个模型的偏差相似， 但方差显著降低。</mark>
##### 7.2.2 并行化 (Parallelization):
- **优势**: Bagging 和 Pasting 的预测器可以 *并行训练* 和 *并行预测*， 这使得它们极适合扩展到大数据集和多核 CPU 的场景。
##### 7.2.3 Scikit-Learn 的 Bagging 和 Pasting:
- `BaggingClassifier` 类可以用于训练 Bagging 集成分类器， `BaggingRegressor` 则用于训练 Bagging 集成回归器。
- 可以通过设置 `bootstrap` 参数为 `True` 或 `False` 来选择使用 `Bagging` 或 `Pasting`。 `n_jobs` 参数用于指定使用多少个 CPU 内核进行训练和预测。
- 如果基本分类器可以估计类别概率， `BaggingClassifier` 会自动执行 *软投票* 而不是 硬投票.
##### 7.2.4 包外评估 (Out-of-Bag Evaluation):
- 原理: 由于 **Bagging** 会放回样本， 有些实例可能会被采样多次，而有些实例则可能根本不被采样，因此每个模型实际平均只使用了 63.2% 的训练样本。 未被使用的 37% 样本被称为 **"包外 (Out-of-Bag)" 样本**。
- 优势: 我们可以利用 **包外样本** 来评估模型的泛化性能， 而*无需额外的验证集。
- 实现: Scikit-Learn 中的 `BaggingClassifier` 类可以通过设置 `oob_score=True` 来进行包外评估。
- 获取包外评估结果: `oob_score_ `是 `BaggingClassifier` 对象的属性， 可以直接访问包外评估的结果。
- 每个样本的包外决策函数: `oob_decision_function_` 属性则包含每个样本在包外评估中的预测概率。

#### 7.3 随机补丁和随机子空间
- 目标: 通过对特征进行采样， 提高集成学习模型的 **多样性** 和 **泛化能力**。
- 特征采样:
	- `max_features` 参数控制为每个预测器选择多少个特征。
	- `bootstrap_features` 参数控制是否放回样本， 类似于 `bootstrap` 参数.
- 两种技术:
	- **随机补丁 (Random Patches)**: 对<mark style="background: #ABF7F7A6;">特征和样本同时进行采样</mark>，每个模型训练使用特征和样本的随机子集。
	- **随机子空间 (Random Subspace)**: 对所有样本进行训练 (即 bootstrap=False 和 max_samples=1.0）， 但<mark style="background: #ABF7F7A6;">对特征进行采样</mark>。
- 优势: 对特征进行采样可以增加预测器之间的多样性， 降低方差， 提高模型的泛化能力。
#### 7.4 随机森林
- 定义: 随机森林是一个由**多棵决策树**组成的集成， 通常用 *Bagging* (有时也可能用 Pasting ) 来训练。
- 参数: `max_samples` 参数控制每个树的训练<mark style="background: #ABF7F7A6;">样本数量</mark> (类似于 `BaggingClassifier` 的 `max_samples` 参数 ), `max_features` 参数控制每个树使用的<mark style="background: #ABF7F7A6;">特征数量</mark> (类似于 `BaggingClassifier` 的 `max_features` 参数 ).
##### 7.4.1 RandomForestClassifier 类 (RandomForestClassifier Class):
- `Scikit-Learn` 提供了 `RandomForestClassifier` 类来创建随机森林分类器， 它整合了 `Bagging` 和 `决策树` 的优点，并进行了一些优化。
- 同时， `RandomForestRegressor` 类则用于回归任务。
- 它比手动使用 BaggingClassifier 和 DecisionTreeClassifier 组合起来更方便，效率更高。
##### 7.4.2 随机森林中的随机性:
- **随机森林** 在 **树的生长** 中引入了 **更多的随机性** :
	- 每个节点分裂时， 不再搜索 *所有特征* 中最优的特征， 而是随机选择 *一部分特征* 进行搜索。
- 优点: 这使得决策树更加多样化， 可以用<mark style="background: #ABF7F7A6;">更高的 偏差 换取更低的 方差</mark>, 最终得到更强大的模型。
##### 7.4.3 极端随机树 (Extremely Randomized Trees)
- 定义: 一种随机性更强的随机森林， 它在每个节点分裂时， 不仅随机选择特征， 还对每个特征使用 *随机阈值* (而不是使用最优阈值， 就像常规决策树一样)。
- 优点: 极端随机树训练速度更快， 因为它省去了在每个节点上寻找最佳阈值这一步骤。
- `ExtraTreesClassifier` 和 `ExtraTreesRegressor` 类: Scikit-Learn 提供了 ExtraTreesClassifier 和 ExtraTreesRegressor 类来创建极端随机树模型。
- 选择: 在实际应用中， 很难预测哪种随机森林更合适。 通常需要尝试 `RandomForestClassifier` 和 `ExtraTreesClassifier` 两种方法， 然后根据交叉验证结果进行选择。
##### 7.4.4 特征重要性 (Feature Importance):
- **定义**: 随机森林能够自动计算每个特征的重要性， 它通过一个 *加权平均* 来计算每个 特征对 *节点不纯度降低* 的贡献。
- 获取特征重要性: 可以通过 `feature_importances_` 属性来访问每个特征的重要性分数。
- 应用: 特征重要性可以帮助你了解哪些特征对模型的预测结果更重要， 并帮助你进行特征选择。
#### 7.5 提升法 Boosting
- 定义: Boosting 是一类集成学习算法， 它以迭代的方式训练多个弱学习器 (如 Decision Tree ), 并将每个弱学习器的结果进行 **加权组合 (Weighted Combination)** ， 形成最终的强学习器。
- 核心思想: Boosting 的原理是 **"关注错误， 提升不足"** ，它在每次迭代中， 都会关注上一次迭代中预测错误的样本， 并重点训练一个新的弱学习器来修正这些错误
- 主要优点: Boosting 可以显著提高模型的泛化能力， 尤其适合处理 *复杂数据集*
##### 7.5.1 AdaBoost 
- 图 7-7: 展示了 AdaBoost 算法 **核心思想** : 将*权重赋予不同样本*， 并使用这些权重来训练 Decision Tree， 最后将多个树的预测结果进行加权组合。![[Pasted image 20250117021052.png]]
- 算法步骤:
	- 第一步， 将每个样本赋予*相同的权重*。
	- 第二步， 训练一个 Decision Tree， 并根据其预测结果调整样本的权重：*误分类样本的权重会提高， 而正确分类样本的权重会降低*
	- 第三步， 使用*更新后的权重*训练下一个 Decision Tree ， 如此反复。
	- 最后， 将每个 Decision Tree 的预测结果进行加权组合， 其中权重与每个 Decision Tree 的训练性能相关。
![[Pasted image 20250117021910.png]]
- 公式 7-1 & 7-2: 展示了 AdaBoost 算法中权重更新的公式
![[Pasted image 20250117021140.png|400]]![[Pasted image 20250117021154.png|400]]![[Pasted image 20250117021201.png|400]]
##### 7.5.2 Gradient Boosting (梯度提升)
- `Gradient Boosting` 是一种更通用的 `Boosting` 方法。 它使用 *梯度下降法* 来更新每个 `Decision Tree` 的权重。 可以用于 *分类* 和 *回归* 任务。
- 核心思想:
	- Gradient Boosting 算法的 **核心思想**: 初始的 Decision Tree 用来预测原始数据。 接下来， <mark style="background: #ABF7F7A6;">每个树的 gradient 都是上一步残差的 gradient (也就是模型的预测值与真实值之间的差距)</mark>
	- Gradient Boosting 算法不断训练 new model 来弥补上一步的残差， 然后将所有模型的预测结果加权求和， 得到最终的 prediction.
- `GradientBoostingClassifier` 和 `GradientBoostingRegressor` 类: Scikit-Learn 提供了 GradientBoostingClassifier 和 GradientBoostingRegressor 类来实现 Gradient Boosting 算法。
- `learning_rate` 参数: 控制每个 Decision Tree 对最终模型的贡献程度。
##### 7.5.3 XGBoost (Extreme Gradient Boosting) 
- XGBoost 是 Gradient Boosting 的 高效实现, 它包含了许多 优化技巧, 例如：
	- 正则化: 防止模型过拟合。
	- 稀疏性处理: 处理特征缺失时更加有效。
	- 并行化: 加速训练。
- XGBoost 通常比普通的 Gradient Boosting 模型具有更高的准确率， 并且训练速度更快。
#### 7.6 堆叠法
- **核心思想:** 将多个模型的预测结果作为输入， 训练一个新的模型 (称为混合器) 來进行最终预测。
- **图 7-12:** 展示了堆叠法的 **核心思想**: 将多个模型的预测结果组合起来， 作为新的特征输入， 训练一个混合模型来得到最终的预测结果。![[Pasted image 20250117033550.png]]
##### 7.6.1 训练过程:
- **图 7-13 和 7-14:** 展示了 **训练堆叠法模型** 的流程。
    1. 首先， 将训练数据分成 **两部分:** 一部分用作**第一层模型** 的训练集， 一部分用作 **留存集 (Holdout Set)**.
    2. 使用第一部分数据训练 **第一层模型**。
    3. **用第一层模型在留存集上预测**, 并将预测结果作为 **新的特征** 加入到留存集中。
    4. **用新的特征集和留存集的目标值** 来训练 **混合器模型 (Blender)**.
##### 7.6.2 多层堆叠:
- **图 7-15:** 说明了可以**重复使用堆叠法** 来构建 **多层混合器**。 例如， 可以先训练一层模型， 然后使用这层模型的预测結果训练第二层模型， 再用这层模型的预测結果训练第三层模型， 依次类推。
##### 7.6.3 `Scikit-Learn` 中的实现:
- `Scikit-Learn` **不支持直接实现堆叠法**， 需要自行实现或使用第三方库 (如 `DESlib`) 来实现。
#### 7.7 练习题
*1.如果你已经在完全相同的训练集上训练了5个不同的模型，并且它们都达到了95%的准确率，是否还有机会通过结合这些模型来获得更好的结果？如果可以，该怎么做？如果不行，为什么？

如果你已经训练了5个不同的模型，并且都达到了95%的精度，则 可以尝试将它们组合成一个投票集成，这通常会带来更好的结果。如果 模型之间非常不同（例如，一个SVM分类器、一个决策树分类器，以及 一个Logistic回归分类器等），则效果更优。如果它们是在不同的训练 实例（这是bagging和pasting集成的关键点）上完成训练，那就更好 了，但如果不是，只要模型非常不同，这个集成仍然有效。

*2.硬投票分类器和软投票分类器有什么区别？

硬投票分类器只是统计每个分类器的投票，然后挑选出得票最多的类。软投票分类器计算出每个类的平均估算概率，然后选出概率最高的类别。它比硬投票法的表现更优，因为它给予那些高度自信的投票更高的权重。但是它要求每个分类器都能够估算出类别概率才可以正常工作（例如，Scikit-Learn中的SVM分类器必须要设置
probability=True）。

*3.是否可以通过在多个服务器上并行来加速bagging集成的训练？pasting集成呢？boosting集成呢？随机森林或stacking集成呢？

对于bagging集成来说，将其分布在多个服务器上能够有效加速 训练过程，因为集成中的每个预测器都是独立工作的。同理，对于 pasting集成和随机森林来说也是如此。但是，boosting集成的每个预 测器都是基于其前序的结果，因此训练过程必须是有序的，将其分布在 多个服务器上毫无意义。对于stacking集成来说，某个指定层的预测器 之间彼此独立，因而可以在多台服务器上并行训练，但是，某一层的预 测器只能在其前一层的预测器全部训练完成之后才能开始训练。

*4.包外评估的好处是什么？

包外评估可以对bagging集成中的每个预测器使用其未经训练的实例（它们是被保留的）进行评估。不需要额外的验证集，就可以对集成实施相当公正的评估。所以，如果训练使用的实例越多，集成的性能可以略有提升。

*5.是什么让极端随机树比一般随机森林更加随机？这部分增加的随机性有什么用？极端随机树比一般随机森林快还是慢？

随机森林在生长过程中，每个节点的分裂仅考虑到了特征的一个随机子集。极限随机树也是如此，它甚至走得更远：常规决策树会搜索出特征的最佳阈值，极端随机树直接对每个特征使用随机阈值。这种极端随机性就像是一种正则化的形式：如果随机森林过拟合训练数据，那么极端随机树可能执行效果更好。而且，由于极端随机树不需要计算最佳阈值，因此它训练起来比随机森林快得多。但是，在做预测的时候，相比随机森林它不快也不慢。

*6.如果你的AdaBoost集成对训练数据欠拟合，你应该调整哪些超参数？怎么调整？

如果你的AdaBoost集成欠拟合训练集，可以尝试提升估算器的数量或是降低基础估算器的正则化超参数。你也可以尝试略微提升学习率。

*7.如果你的梯度提升集成对训练集过拟合，你是应该提升还是降低学习率？

如果你的梯度提升集成过拟合训练集，你应该试着降低学习率，也可以通过提前停止法来寻找合适的预测器数量（可能是因为预测器太多）。

*8.加载MNIST数据集（第3章中有介绍），将其分为一个训练集、一个验证集和一个测试集（例如，使用50 000个实例训练、10 000个实例验证、10 000个实例测试）。然后训练多个分类器，比如一个随机森林分类器、一个极端随机树分类器和一个SVM分类器。接下来，尝试使用软投票法或者硬投票法将它们组合成一个集成，这个集成在验证集上的表现要胜过它们各自单独的表现。成功找到集成后，在测试集上测试。与单个的分类器相比，它的性能要好多少？

*9.运行练习题8中的单个分类器，用验证集进行预测，然后用预测结果创建一个新的训练集：新训练集中的每个实例都是一个向量，这个向量包含所有分类器对于一张图像的一组预测，目标值是图像的类。恭喜，你成功训练了一个混合器，结合第一层的分类器，它们一起构成了一个stacking集成。现在在测试集上评估这个集成。对于测试集中的每张图像，使用所有的分类器进行预测，然后将预测结果提供给混合器，得到集成的预测。与前面训练的投票分类器相比，这个集成的结果如何？


### 第八章 降维
- 介绍了 **维度灾难 (Curse of Dimensionality)** 的概念。
- 简述了在高维空间中， 数据具有 **稀疏性** 和 **不规则性** 的特点， 从而导致机器学习模型很难有效地泛化到新数据，
- 描述了 **降维 (Dimensionality Reduction) 技术** 的意义， 它能够有效地解决 **高维数据** 带来的各种问题。
- 解释了降维技术可以在 **加快训练速度**、 **提高模型泛化能力**、 **方便数据可视化** 等方面发挥重要作用。
- 最后指出， 尽管降维技术有很多优点， 但它也是一个 **“权衡”(trade-off) 问题** : 它可能 **损失一些信息** ， 并会使数据处理 **更加复杂**。
#### 8.1 维度的诅咒
- 深入解释了 **维度的诅咒** 产生的根本原因: **高维空间的独特特性**, 它与我们熟悉的低维空间完全不同.
- 描述了 **高维空间中的稀疏现象** - 随机选取的两个数据点很可能离得很远，这使得模型不容易找到数据之间的 **模式**。
- 解释了高维数据会给机器学习带来的 **负面影响** , 例如:
    - **模型过擬合**
    - **训练数据需求指数增长**
    - **数据稀疏性** 导致模型无法在训练集上找到足够的规律
#### 8.2 降维的主要方法
- 目的: 降低数据的维度，解决高维数据的 维度诅咒 问题。
##### 8.2.1 投影 (Projection)
- **基本思想**: 假设 <mark style="background: #FFB8EBA6;">数据点 集中在 高维空间中的低维子空间</mark> 内，可以通过 *投影* 将数据点 *映射*到 这个低维子空间
- 图 8-2: 展示了 3D 数据集中， 数据点 集中在一个 2D 平面 附近的示例。![[Pasted image 20250117200451.png]]
- 图 8-3: 说明了 将数据点投影到平面上的过程 ， 降低了 维度。![[Pasted image 20250117200504.png]]
- **局限性**:
	- **非线性数据**: 如果 *子空间发生扭曲* (例如，瑞士卷数据集) , 简单的投影就可能丢失很多信息， 难以得到好的降维结果。![[Pasted image 20250117201416.png]]
	- 图 8-5: 说明了 *简单投影到平面上* 可能会 *压缩数据*， 而 *展开数据* 则需要 *非线性方法*。
##### 8.2.2 流形学习 (Manifold Learning)
- **基本思想**: 假设数据点*分布*在 一个 *低维流形 (Manifold)* 上，该流形在 *高维空间中弯曲和扭曲。
- **“流形假设”**: 许多 *现实世界中的数据* 都具有 **流形结构 (Manifold Structure)** ， 即 *局部线性 ， 但 全局非线性*。
- 图 8-4: 展示了 瑞士卷数据集 的 流形结构 ， 它是一个 二维流形, 在 三维空間中扭曲。
- **目标**: 使用 *非线性方法 将数据 映射到 其 低维流形* 上的表示。
- 图 8-6: 说明了 *使用流形学习可能简化模型效果* ， 例如， 非线性数据的 决策边界 在低维流形上可能变得 更简单。
- 但 **并非所有任务都适合流形学习** ， 有时在 **高维空间中 可能 更简单**。![[Pasted image 20250117201724.png]]
#### 8.3 PCA(主成分分析)
##### 8.3.1 保持差异性
- **目标**: PCA 的目标是找到一个 <mark style="background: #FFB8EBA6;">最靠近数据点的低维超平面</mark>， 并将数据投影到该超平面上， 以<mark style="background: #FFB8EBA6;">最大限度地保留数据中的 差异性 (variance)</mark>。
- 图 8-7: 展示了 2D 数据集中不同投影方向对 数据差异性 的保留情况。![[Pasted image 20250117202417.png]]
	- 沿 实线 投影保留了最多的差异性。
	- 沿 点线 投影保留了最少的差异性。
	- 沿 虚线 投影保留了中间的差异性。
	- **选取原则**: PCA 会选取 *能够最好地保留数据方差的轴* 作为*投影方向* 。
##### 8.3.2 主要成分
- 定义: <mark style="background: #FFB8EBA6;">数据的主要成分 (Principal Components, PC</mark>) 是数据中 **差异性最大的方向** ， 由 **单位向量** 表示。
- 图 8-7: 展示了 2D 数据集中 前两个主要成分 (PC1 和 PC2)。
- **选择策略**: PCA 找到 *所有主要成分 (数量等于数据的维度)*， 并按其 *保留的方差大小排序 。
- **方向变化**: PC 向量 的 方向 可能会发生变化， 但 它们所在的空间通常保持一致。
##### 8.3.3 奇异值分解 (SVD)
- **SVD 是一个重要的矩阵分解技术**: 它可以将 *数据矩阵 X 分解成 三个矩阵 的乘积： UΣVT*
- 公式 8-1: 展示了 SVD 的公式。![[Pasted image 20250117202828.png]]
- 意义: *V 矩阵包含所有 主要成分向量， 可以用来获得数据的 主要成分方向*。
- 代码示例: 代码示例了使用 NumPy 库进行 SVD 分解， 并提取 前两个主成分向量 ( c1 和 c2)。
```python
X_centered = X - X.mean(axis=0) 
U, s, Vt = np.linalg.svd(X_centered) 
c1 = Vt.T[:, 0] 
c2 = Vt.T[:, 1]
```
##### 8.3.4 向下投影
- 目标: 将 **高维数据** 投影到 *前 d 个主要成分* 所张成的 *超平面 (低维空间)* 上， 从而降低数据的维度。
- 公式 8-2: 提供了 投影公式: $X_{d-proj} = X * W_{d}$ ， 其中 $W_d$ 是 *V 矩阵的前 d 列*， 用来保留 最多信息 的维度。![[Pasted image 20250117204028.png]]
- 代码示例: 代码示例了将 3D 数据投影到 2D 平面上， 并得到新的 2D 数据 X2D。
```python
W2 = Vt.T[:, :2] 
X2D = X_centered.dot(W2)
```
##### 8.3.5 Scikit-Learn 中的 PCA 类
- **PCA 类**: Scikit-Learn 提供了一个方便的 PCA 类， 它可以自动处理数据 居中 、 并进行 SVD 分解。
- `n_components`: 参数用来指定需要保留的主成分数量 
	- **整数**: 保留几个 PC
	- **浮点数**: 保留多少 **方差比例**
- `components_ `属性: 包含 主成分向量 (对应 Wd 的转置).
##### 8.3.6 可解释方差比例 (Explained Variance Ratio)
- 重要指标:` explained_variance_ratio_` 属性可以用来查看每个 *主要成分所保留的方差比例* (即 **信息量** ）。
- 意义: 可以帮助你有效地选择 **合适的 d 值 (即降维后的维度)**
##### 8.3.7 压缩 (Compression)
- **PCA 可以用来压缩数据**: 将 *高维数据* 压缩为 *低维数据 (保留较多的方差)*, 然后可以 **反向解压缩 (但会损失信息， 无法完全恢复原始数据) 。
- 代码示例: 代码示例了将 MNIST 数据集 进行压缩， 並展示了 压缩前后的数据对比。![[Pasted image 20250117204511.png]]
- 公式 8-3： 展示了 PCA 逆变换 公式![[Pasted image 20250117204415.png]]
##### 8.3.8 随机 PCA (Randomized PCA)
- `svd_solver="randomized"` :  可以使用 *随机 PCA 算法* 来 *加速 PCA 计算* ， 尤其是 *当数据维度很高时*。
- **优势**: 随机 PCA 的 *计算复杂度* <mark style="background: #FFB8EBA6;">低于</mark>传统的*完全 SVD 方法*。
- `Scikit-Learn` 在 `svd_solver="auto"` 设置 下， 会根据数据的 *维度* 和 *目标维度* 自动选择用 *随机 PCA* 还是 *完整 SVD 方法*。
- 可以手动设置 `svd_solver` 选择 完整 SVD ("full") 或 随机 PCA ("randomized") 方法
##### 8.3.9 增量 PCA (Incremental PCA)
- `IncrementalPCA` 类: `Scikit-Learn` 中的 `IncrementalPCA 类` 可以<mark style="background: #ABF7F7A6;"> 逐步处理训练数据 (例如分批处理)</mark>  , 尤其适用于 <mark style="background: #ABF7F7A6;">大型数据集</mark> 或 <mark style="background: #ABF7F7A6;">运行时间较长的系统</mark> 。
- `partial_fit()` 方法: 该方法用于 *更新模型参数*， 无需将所有数据加载到内存中。
- `memmap` 类: 可以用来 *处理存储在磁盘上的大型数据* (例如， MNIST 数据集), 将文件当作 *内存数组*: 每次只加载需要读取的 *小部分数据* , 减少内存占用
#### 8.4 内核PCA（Kernel PCA）
- **目标**: 利用 **内核技巧 (Kernel Trick)** ， 将数据 *隐式* 映射到 *高维特征空间 (Feature Space)* 中， 然后在 *特征空间* 中进行线性 PCA ， 这样可以实现 *非线性降维*。
- **优势**: kPCA 适用于 **流形数据** (例如 瑞士卷数据集) 的降维， 可以 **更好地保留数据的聚类结构**。
- `KernelPCA 类`: Scikit-Learn 提供了 KernelPCA 类， 支持 *不同的内核 (例如 RBF 内核，sigmoid 内核)*, 可以通过 **gamma 参数** 调整内核的 **宽度** 。
##### 8.4.1 选择合适的内核 (Choosing the Right Kernel)
- **挑战**: 由于 `kPCA` 属于 *无监督学习* ， 选择合适的 *内核* 和 *超参数* (例如 gamma) 需要 **借助于其他任务 (例如， 分类任务) 的性能指标**
	- 例如， 使用GridSearchCV 方法， 寻找在 分类任务 上性能最好的 内核 和 超参数组合。
- 无监督方法: 可以采用 **重构误差 (Reconstruction Error)** 作为 **评估指标** , 选择产生 *最小重构误差* 的内核和超参数。
##### 8.4.2 重构误差 (Reconstruction Error)
- **挑战**: 由于 *特征空间* 是*高维*的 (甚至 无限维)，<mark style="background: #FFB8EBA6;"> 无法直接 计算重构点 ，无法直接计算 重构误差。</mark>
- 解决方案:
	- 找到原始空间 中一个点 ( **重构原像**), 它被 *映射到 kPCA 后的重建点* 附近。
	- 然后计算 <mark style="background: #ADCCFFA6;">重构原像 与 原始数据点 之间的 平方距离 作为 重构误差</mark>。
##### 8.4.3 重构原像 (Pre-image)
- **思路**: 可以使用 **有监督学习** (例如 回归模型):
	- 将 *kPCA 投影后的实例* 作为 *训练集* 。
	- 将 *原始实例* 作为 *目标值* 。
- `fit_inverse_transform=True`: Scikit-Learn 提供了一个便利的 设置， 可以让 `KernelPCA 类` 自动训练 一个 回归模型 ， 用来进行 **反向映射** 。
- `inverse_transform()` 方法: 可以用来 *计算重构原像*
#### 8.5 局部线性嵌入 LLE
- **目标**: LLE 是一种 *流形学习方法* ， 它尝试 *保留数据点的局部线性关系* ; 它*不使用投影*
- 工作原理:
	- **计算每个点的局部线性关系**: 计算每个点与其 k 个最近邻居 (c.n.) 之间的 *线性关系* (找到线性权重， 使得点能够用邻居的线性组合来表示)。
	- **寻找低维空间表示**: 找出 *低维空间中 每个点的 最佳位置* ， 使得 *保持原始空间中的局部线性关系*
- 优势: LLE 擅长展开 **扭曲的流形**, 并且 **对噪声不敏感**。
- 代码示例: 代码示例了如何使用 Scikit-Learn 中的 `LocallyLinearEmbedding` 类来展开 瑞士卷 数据集.
- LLE 的步骤: 
	- 第一步 (公式 8-4):
		- 找到每个点与 k 个最近邻居的 线性权重 ， 称为 W 。
		- 目标: <mark style="background: #ABF7F7A6;">最小化 重建误差</mark> (每个点用邻居的线性组合表示的误差)。![[Pasted image 20250117213708.png]]
	- 第二步 (公式 8-5):
		- 将数据 映射 到 d 维 低维空间 (其中 d 小于原始数据的维度), 得到 z(i) 
		- 目标: 在低维空间中 尽量保持 W 所表示的 局部线性关系 ， 即 <mark style="background: #ABF7F7A6;">最小化 在低维空间中的 重建误差</mark>。![[Pasted image 20250117213718.png]]
- 算法复杂度: `LLE` 算法的计算复杂度很高， 尤其是 `O(m2)` 部分会使它难以处理 超级大数据集。
#### 8.6 其他降维技术
- **随机投影 (Random Projection)** : 使用随机的 *线性投影* 来降低数据维度
- **多维缩放 (MDS)**: 尝试 *保留数据点之间的距离* 来降低维度。
- **等距映射 (Isomap)**: 构建一个*图模型* (将每个点与其最近邻居连接起來)， 然后在 图模型 中， 考虑两个点的 *测地距离 (Geodesic Distance)* (即 从*一个点到另一个点的最短路径上的距离*)， 进行降维。
- **t-SNE (t - 分布随机邻域嵌入)** : 将 *相似的点* 映射到 *低维空间中相互靠近的位置*， 将 *不相似* 的点映射到 *低维空间中相互远离的位置*, 主要用于 数据可视化, 尤其是 *高维数据的聚类可视化*。
- **线性判别分析 (LDA)**: 是一种 *分类*算法， 但它会 *找到类别之间最有区别的轴*， 这些轴可以用来*定义投影方向* ， 从而更容易 *分离不同类别*。
#### 8.7 练习题
*1.减少数据集维度的主要动机是什么？主要缺点是什么？
![[Pasted image 20250118051607.png|450]]

*2.维度的诅咒是什么？

维度的诅咒是指许多在低维空间中不存在的问题，在高维空间中发生。在机器学习领域，一个常见的现象是随机抽样的高维向量通常非常稀疏，提升了过拟合的风险，同时也使得在没有充足训练数据的情况下，要识别数据中的模式非常困难

*3.一旦降低了数据集的维度，是否可以逆操作？如果可以，怎么做？如果不能，为什么？

一旦使用我们讨论的任意算法减少了数据集的维度，就几乎不可能再将操作完美地逆转，因为在降维过程中必然丢失了一部分信息。此外，虽然有一些算法（例如PCA）拥有简单的逆转换过程，可以重建出与原始数据集相似的数据集，但是也有一些算法不能实现逆转（例如TSNE）。

*4.可以使用PCA来减少高度非线性的数据集的维度吗？

对大多数数据集来说，PCA可以用来进行显著降维，即便是高度非线性的数据集，因为它至少可以消除无用的维度。但是如果不存在无用的维度（例如瑞士卷），那么使用PCA降维将会损失太多信息。你希望的是将瑞士卷展开，而不是将其压扁。

*5.假设你在1000维的数据集上执行PCA，将可解释方差比设置为95%。结果数据集将具有多少个维度？

这是个不好回答的问题，它取决于数据集。我们来看看两个极端的示例。首先，假设数据集是由几乎完全对齐的点组成的，在这种情况下，PCA可以将数据集降至一维，同时保留95%的方差。现在，试想数据集由完全随机的点组成，分散在1000个维度上，在这种情况下，需要在950个维度上保留95%的方差。所以，这个问题的答案是：取决于数据集，它可能是1到950之间的任何数字。将解释方差绘制成关于维度数量的函数，可以对数据集的内在维度获得一个粗略的概念。

*6.在什么情况下，你将使用常规PCA、增量PCA、随机PCA或内核PCA？

常规PCA是默认选择，但是它仅适用于内存足够处理训练集的时候。增量PCA对于内存无法支持的大型数据集非常有用，但是它比常规PCA要慢一些，所以如果内存能够支持，还是应该使用常规PCA。当你需要随时应用PCA来处理每次新增的实例时，增量PCA对于在线任务同样有用。当你想大大降低维度数量，并且内存能够支持数据集时，使用随机PCA非常有效，它比常规PCA快得多。最后，对于非线性数据集，使用核化PCA非常有效。

*7.如何评估数据集中的降维算法的性能？

直观来说，如果降维算法能够消除许多维度并且不会丢失太多信息，那么这就算一个好的降维算法。进行衡量的方法之一是应用逆转换然后测量重建误差。然而并不是所有的降维算法都提供了逆转换。还有另一种选择，如果你将降维当作一个预处理过程，用在其他机器学习算法（比如随机森林分类器）之前，那么可以通过简单测量第二个算法的性能来进行评估。如果降维过程没有损失太多信息，那么第二个算法的
性能应该跟使用原始数据集一样好。

*8.链接两个不同的降维算法是否有意义？

链接两个不同的降维算法绝对是有意义的。常见的示例是使用PCA快速去除大量无用的维度，然后应用另一种更慢的降维算法，如LLE。这种两步走的策略产生的结果可能与仅使用LLE相同，但是时间要短得多。

*9.加载MNIST数据集（在第3章中介绍），并将其分为训练集和测试集（使用前60 000个实例进行训练，其余10 000个进行测试）。在数据集上训练随机森林分类器，花费多长时间，然后在测试集上评估模型。接下来，使用PCA来减少数据集的维度，可解释方差率为95%。在精简后的数据集上训练新的随机森林分类器，查看花费了多长时间。训练速度提高了吗？接下来，评估测试集上的分类器。与之前的分类器相比如何？

*10.使用t-SNE将MNIST数据集降至两个维度，然后用Matplotlib绘制结果。你可以通过散点图用10个不同的颜色来代表每个图像的目标类，或者也可以用对应实例的类（从0到9的数字）替换散点图中的每个点，甚至你还可以绘制数字图像本身的缩小版（如果你绘制所有数字，视觉效果会太凌乱，所以你要么绘制一个随机样本，要么选择单个实例，但是这个实例的周围最好没有其他绘制的实例）。现在你应该得到了一个很好的可视化结果及各自分开的数字集群。尝试使用其他降维算法，如PCA、LLE或MDS等，比较可视化结果。

### 第九章 无监督学习技术
- 无监督学习 (Unsupervised Learning)
	- 定义: 无监督学习是指在数据没有标签的情况下，从数据中发现隐藏的模式和结构。
	- 重要性: Yann LeCun 将无监督学习比喻为 “蛋糕本体”， 强调它在人工智能研究中的巨大潜力。
	- 现实问题: 现实数据中， 无标签数据远多于有标签数据， 如何充分利用这些数据是一项重要的挑战。
	- 示例: 生产线上的缺陷产品检测， 如果需要人工标记每一张图片， 成本和工作量都会很高。 如果可以用 无监督学习 来分析数据， 可以有效地提高效率。
- 无监督学习任务
	- 本章将重点介绍几种常见的无监督学习任务:
		- **聚类 (Clustering)** : 将 相似的实例 分组到 **同一个簇 (Cluster)** 中。
		- **异常检测 (Anomaly Detection)** : 学习数据的 正常模式， 然后识别 与正常模式不符的异常数据。
		- **密度估计 (Density Estimation)**: 估计数据生成 *概率密度函数 (PDF)* ， 用于 异常检测 ( 因为异常数据通常位于 *密度很低的区域*), 以及 数据分析和可视化。
- 本章概述
	- 本章将会讲解 *K-Means, DBSCAN* 等 *聚类算法*。
	- 同时也会介绍 *高斯混合模型 (GMM)*, 以及它的 应用 :
		- 密度估计
		- 聚类
		- 异常检测
#### 9.1 聚类
- 定义: 将数据集中相似的实例划分到簇中。
- 应用: 广泛应用于客户细分，数据分析，降维，异常检测，半监督学习，搜索引擎，图像分割等。
- 不同算法的差异: 不同的聚类算法对数据分布和集群的形状有不同的假设，因此会得到不同的结果。
##### 9.1.1 K-Means
- 优点: 简单，速度快，可扩展。
- 算法流程:
	- 1.随机初始化中心点 (k 个中心点， 代表 k 个簇)。
	- 2.将每个实例分配到距离其最近的中心点的簇中。
	- 3.更新每个簇的中心点， 用<mark style="background: #ADCCFFA6;">簇中所有实例的均值</mark>来代替。
	- 4.重复步骤 2&3， 直到中心点不再改变。
- 局限性:
	- 只能处理 *球形形状的*簇， 对 *不同大小、不同密度、非球形* 的簇难以准确识别。
	- 需要 *预先指定簇的数量 (k)*， 难以自动确定最佳的 k 值。
	- 中心点初始化会影响 最终结果， 可能陷入 *局部最优解*。
##### 9.1.2 K-Means 算法改进
- **中心点初始化方法:
	- **K-Means++**： 利用 更智能的初始化方法 ，选择 *距离较远的点* 作为中心点， 提高找到 *全局最优解* 的可能性.
- **加速策略:
	- **Elkan 算法 (默认)**: 利用 *三角不等式* 优化 *距离计算*， 加速 K-Means 。
	- **小批量 K-Means (MiniBatchKMeans)**: 一次只处理一小部分数据， 提高 训练速度， 非常适合处理 无法完全加载到内存的大型数据集 。
##### 9.1.3 寻找最佳集群数量 (k)
- **"肘" 方法**: 画出 *惯性 (Inertia) (或者说平方误差) 随着 k 变化的曲线*， 在曲线*拐点*的地方通常是 k 的最佳选择。
- **轮廓分数 (Silhouette Score)**: 计算<mark style="background: #ADCCFFA6;">每个点对其所属簇和距离其最近的另一个簇之间的 相似度 ， 并取 平均值</mark>。
	- 高的轮廓分数表明数据点很好地聚集在 正确的簇 中。
	- 可以通过视察 **轮廓图** 来判断 k 的选择是否合适。
##### 9.1.4 K-Means 应用:
- **图像分割**: 将图像分成多个区域 (每个区域里的像素颜色较为一致).
- **数据预处理**: 将 高维数据 进行 降维 , 可以通过 聚类 将每个数据点替换成它所属 簇的中心点。
##### 9.1.5 半监督学习 (Semi-Supervised Learning)
- **标签传播 (Label Propagation)**: 使用 少数已标记数据, 通过聚类 *将标签传播到同一个簇的其他数据* ， 扩大带标签数据规模。
- **主动学习**: 主动地 *选择对模型最有帮助的数据* ， 让 *人工标注* ， 可以更高效地提高模型的准确性。
##### 9.1.6 DBSCAN (密度可达空间聚类算法)
- 工作原理:
	- 计算 **密度**: 每个点在其 `eps - 邻居`中包含多少个点。
	- 确定 **核心点**: 具有 *足够多邻居* 的点。
	- 将 **相互连接的核心点 以及 它们周围的点** 组成一个簇。
	- 没有被分配到任何集群的点被视为 *异常点*。
- 优势:
	- 可以 识别*任意形状* 的 *不同大小*的簇。
	- 对 *噪声和异常点 不敏感*。
- 参数:
	- `eps` : 邻域半径
	- `min_samples` : 核心点 最小邻居数量
- 挑战: 如果数据的 *密度变化较大*， 效果可能会不好。
- **DBSCAN 没有 predict() 方法**: 需要额外训练 分类器 来预测 新数据点 所属的簇。
##### 9.1.7 其他聚类算法
- **层次聚类 (Agglomerative Clustering)**: 按照 *层次结构* (不断合并较小的簇) 来构建簇，
	- 优势: 可以识别 *不规则形状的簇*, 易于扩展到大量数据， 不需要预设 k 值 (但是需要进行细 *粒度调整 (resolution)* ).
- **BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)**: 针对 *大型数据集* 设计，
	- 优势: 比 K-Means 更快，且结果相似.
- **均值漂移 (MeanShift)**: 从每个点开始迭代地 *移动圆圈* (直到圆圈中心不再移动)，
	- 优势: 可以识别任何形状的簇， 对 密度变化 不敏感，但 *计算复杂度* 很高， 不适合处理 大型数据集。
- **相似性传播 (Affinity Propagation)**: 利用*投票机制* (每个点投票给其他点， 作为 代表性点 ），
	- 优势: 可以识别不同大小的簇， 但是 *计算复杂度* 很高。
- **谱聚类 (Spectral Clustering)**: 先将数据 *映射到低维空间* (通过 相似度矩阵 ), 然后再用其他聚类算法进行聚类。
	- 优势: 可以识别复杂结构簇， 可以用来 *分割图* (例如， 社交网络图).
#### 9.2 高斯混合模型
- 定义: GMM 假设<mark style="background: #ABF7F7A6;">数据是由多个高斯分布的混合生成</mark>的， `GMM` 能够识别 **不同形状、大小、密度和方向** 的簇。
- 生成过程:
	- 随机选择一个簇 (概率由 **权重 φ(j)** 决定)。
	- 从对应簇的 **高斯分布** 中抽取样本 (该分布的 *均值 μ(j)* 和 *协方差矩阵 Σ(j)* 是模型参数)
- `GaussianMixture` 类: Scikit-Learn 提供了一个 GaussianMixture 类， 可以用来 估计 GMM 的参数。
- 参数:
	- `n_components`: 集群数量 (k)。
	- `n_init`: 初始化次数 （因为 GMM 也可能陷入 *局部最优解*)。
- `EM` 算法 : GMM 模型的训练基于 **期望最大化算法 (Expectation-Maximization, EM)**
	- 类似 K-Means， 但 GMM 使用的是 **软赋值** (每个数据点都可能属于多个簇，概率分布决定).
	- 算法会 *重复迭代两个步骤 :
		- **期望**步骤: 计算<mark style="background: #ABF7F7A6;">每个数据点 属于 每个簇的概率</mark> 。
		- **最大化**步骤: 根据<mark style="background: #ABF7F7A6;"> 每个数据点 属于各簇的 概率 </mark>， 来 <mark style="background: #ABF7F7A6;">更新各个簇的参数 ( μ(j) 和 Σ(j) 和 φ(j)).</mark>
##### 9.2.1 选择聚类数
- K-Means 中可以使用 *惯性 (Inertia)* 和 *轮廓分数 (Silhouette Score)* 来帮助 <mark style="background: #ABF7F7A6;">选择 k 值。</mark>
- GMM 中， 可以选择 *最小化信息量准则*的模型， 例如 **BIC (贝叶斯信息准则)** 或者 **AIC (赤池信息准则)**。
	- 这两个准则都对 *模型复杂度 (参数数量) 进行惩罚*， 同时也鼓励 *模型能很好地拟合数据*
##### 9.2.2 似然函数 (Likelihood Function)
- 定义: 用于 *描述给定数据 (X)* 的情况下， 某组 *模型参数 (θ) 的 合理性*
- 区别: **概率分布 (PDF) 是 数据 (X) 的函数**（参数 θ 是固定的）， 而 **似然函数** 是 **模型参数 (θ) 的函数 (数据 X 是固定的)**
- **最大化似然函数**: 选择<mark style="background: #ABF7F7A6;">最合适的模型参数</mark> ， 使得模型能够<mark style="background: #ADCCFFA6;">最好地 解释观测到的数据</mark> ， 这称为 <mark style="background: #ABF7F7A6;">最大似然估计 (MLE)</mark>。
- 如果考虑参数的 *先验知识*, 可以采用 *最大后验 (MAP) 估计*
##### 9.2.3 贝叶斯高斯混合模型 (Bayesian Gaussian Mixture)
- `BayesianGaussianMixture` 类: Scikit-Learn 提供的 BayesianGaussianMixture 类， 可以 自动选择最合适的集群数量。
- 思路:
	- 将 **集群参数 (μ, Σ, φ)** 也当作 *随机变量* (而不是固定参数), 使用 *贝叶斯推理* (例如使用 *变分推理*) 来 *更新模型参数 。
- 优势:
	- 不需要预先指定集群数量 k。
	- 可以根据数据自动 *调整集群数量*(通过 **权重 φ(j) 接近 0 来判断该簇是否需要)**
- 先验知识: 可以通过 超参数`weight_concentration_prior` 来 *设置对集群数量的先验信念*。
- 变分推理: 使用 *变分参数 (λ) 来 近似后验分布* ， 通过 *最大化 ELBO (Evidence Lower Bound) 来 优化变分参数*， 并 更新模型参数。
- 其他变分推理方法: *黑盒随机变分推理 (BBSVI)* 是 一种更通用的方法， 可以用于 任意类型的模型。
##### 9.2.4 其他异常检测技术
- `PCA + inverse_transform()`: 可以用 PCA 重构数据， *异常数据的重建误差通常会很大*
- `EllipticEnvelope (最小协方差决定 (Fast-MCD))`: 用来*估算高斯分布* (并忽略异常值),
- `IsolationForest (隔离森林)`: 利用 *随机森林* (按随机特征进行树节点分割) 来 将*异常值与正常数据分离* ， 异常值一般会 更容易被分离。
- `LocalOutlierFactor (局部离群因子，LOF)`: 计算 *数据点和邻居点* 的 *密度差异*， 异常点通常 更孤立。
- `OneClassSVM (单类支持向量机)` : 尝试在 *高维空间*中 找到一个 *包含所有正常样本的区域*, 其他样本被判为 *异常*
#### 9.3 练习题
*1.如何定义聚类？你能列举几种聚类算法吗？ 

在机器学习中，聚类是将相似的实例组合在一起的无监督任务。 相似性的概念取决于你手头的任务：例如，在某些情况下，两个附近的 实例将被认为是相似的，而在另一些情况下，只要它们属于同一密度 组，则相似的实例可能相距甚远。流行的聚类算法包括K-Means、 DBSCAN、聚集聚类、BIRCH、均值平移、亲和度传播和光谱聚类

*2.聚类算法的主要应用有哪些？ 

聚类算法的主要应用包括数据分析、客户分组、推荐系统、搜索引擎、图像分割、半监督学习、降维、异常检测和新颖性检测。

*3.描述两种使用K-Means时选择正确数目的集群的技术。 

肘部法则是一种在使用K-Means时选择集群数的简单技术：将惯量（从每个实例到其最近的中心点的均方距离）作为集群数量的函数绘制出来，并找到曲线中惯量停止快速下降的点（“肘”）。另一种方法是将轮廓分数作为集群数量的函数绘制出来。通常最佳集群数是在一个高峰的附近。轮廓分数是所有实例上的平均轮廓系数。对于位于集群内且与其他集群相距甚远的实例，该系数为+1；对于与另一集群非常接近的实例，该系数为-1。你也可以绘制轮廓图并进行更细致的分析。

*4.什么是标签传播？为什么要实施它，如何实现？ 

标记数据集既昂贵又费时。因此，通常有很多未标记的实例，很少有标记的实例。标签传播是一种技术，该技术包括将部分（或全部）标签从已标记的实例复制到相似的未标记实例。这可以大大增加标记实例的数量，从而使监督算法达到更好的性能（这是半监督学习的一种形式）。一种方法是在所有实例上使用诸如K-Means之类的聚类算法，然后为每个集群找到最常见的标签或最具代表性的实例（即最接近中心点的实例）的标签并将其传播到同一集群中未标记的实例。

*5.你能否说出两种可以扩展到大型数据集的聚类算法？两个寻找 高密度区域的算法？

K均值和BIRCH可以很好地扩展到大数据集。DBSCAN和Mean-Shift寻找高密度区域。

*6.你能想到一个主动学习有用的示例吗？你将如 何实施它？ 

当你有大量未标记的实例而做标记非常昂贵时，主动学习就非常
有用。在这种情况下（非常常见），与其随机选择实例来做标记，不如
进行主动学习，这通常是更可取的一种方法，人类专家可以与算法进行
交互，并在算法有需要时为特定实例提供标签。常见的方法是不确定性
采样（见9.1.5节的“主动学习”）。

*7.异常检测和新颖性检测有什么区别？ 

许多人把术语异常检测和新颖性检测互换，但是它们并不完全相同。在异常检测中，算法对可能包含异常值的数据集进行训练，目标通常是识别这些异常值（在训练集中）以及新实例中的异常值。在新颖性检测中，该算法在假定为“干净”的数据集上进行训练，其目的是严格在新实例中检测新颖性。某些算法最适合异常检测（例如隔离森林），而其他算法更适合新颖性检测（例如单类SVM）。

*8.什么是高斯混合模型？你可以将其用于哪些任务？ 

高斯混合模型（GMM）是一种概率模型，它假定实例是由参数未知的多个高斯分布的混合生成的。换句话说，我们假设数据可以分为有限数量的集群，每个集群具有椭圆的形状（但是集群可能具有不同的椭圆形状、大小、方向和密度），而我们不知道每个实例属于哪个簇。该模型可用于密度估计、聚类和异常检测。

*9.使用高斯混合模型时，你能否列举两种技术来找到正确数量的 集群？ 

使用高斯混合模型时，找到正确数量的集群的一种方法是将贝叶斯信息准则（BIC）或赤池信息准则（AIC）作为集群数量的函数绘制出来，然后选择使BIC或AIC最小化的集群数量。另一种技术是使用贝叶斯高斯混合模型，该模型可以自动选择集群数。

*10.经典的Olivetti人脸数据集包含400张灰度的64×64像素的人 脸图像。每个图像被展平为大小为4096的一维向量。40个不同的人被 拍照（每个10次），通常的任务是训练一个模型来预测每个图片中代 表哪个人。使用sklearn.datasets.fetch_olivetti_faces（）函数来 加载数据集，然后将其拆分为训练集、验证集和测试集（请注意，数 据集已缩放到0到1之间）。由于数据集非常小，你可能希望使用分层 抽样来确保每组中每个人的图像数量相同。接下来，使用K-Means对图 像进行聚类，确保你拥有正确的集群数（使用本章中讨论的一种技 术）。可视化集群：你在每个集群中看到了相似面孔吗？

*11.继续使用Olivetti人脸数据集，训练分类器来预测每张图片代 表哪个人，并在验证集上对其进行评估。接下来，将K-Means用作降维 工具，然后在简化集上训练分类器。搜索使分类器获得最佳性能的集 群数量：可以达到什么性能？如果将简化集中的特征附加到原始特征 上（再次搜索最佳数目的集群）怎么办？ 

*12.在Olivetti人脸数据集上训练高斯混合模型。为了加快算法的 速度，你可能应该降低数据集的维度（例如，使用PCA，保留99%的方 差）。使用该模型来生成一些新面孔（使用sample（）方法），并对 其进行可视化（如果使用PCA，需要使用其inverse_transform（）方 法）。尝试修改一些图像（例如，旋转、翻转、变暗），然后查看模 型是否可以检测到异常（即比较score_samples（）方法对正常图像和 异常图像的输出）。 

*13.一些降维技术也可以用于异常检测。例如，使用Olivetti人脸 数据集并使用PCA对其进行降维，保留99%的方差。然后计算每个图像 的重建误差。接下来，使用一些你在练习12中构建的一些修改后的图 像，查看其重建误差：注意重建误差的大小。如果画出重建图像，你 会看到原因：它尝试重建一张正常的脸。